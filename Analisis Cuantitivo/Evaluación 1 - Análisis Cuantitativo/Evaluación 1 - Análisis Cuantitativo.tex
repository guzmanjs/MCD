\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Evaluación 1 - Análisis Cuantitativo}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \emph{Luisa Fernanda Giraldo }

\emph{Juan Sebastian Guzman }

     \textbf{Introducción}

Como parte del ejercicio profesional de los científicos de datos, los
modelos de aprendizaje supervisado están siempre presentes. Dentro de
estos, los modelos de regresión lineal han representado una herramienta
poderosa para comprender la relación entre fenómenos y predecir sucesos,
agragando valor a través de la manera en la que ofrece información para
la toma de decisiones y la gestión de los negocios. Este trabajo se
propone reforzar los conceptos de regresión lineal y afinar las
habilidades de sus autores como científicos de datos, convirtiéndose en
una oportunidad para afianzar conceptos y ponerlos en práctica para
nuestro futuro como profesionales en el campo. Abordaremos modelos de
regresión lineal simple con y sin interacción así como modelos de
regresión múltiple; hallamos los resultados a través de paquetes
estadísticos de python así la aproximación matricial a los cálculos de
los parámetros. Por último, discutiremos conceptualmente sobre el modelo
de regresión logística.

\textbf{Conceptos Claves}: Modelo de regresion lineal, regresión lineal
múltiple, prueba de hipótesis, regresión robusta, modelo con
interacción, regresión logística.

 \textbf{Summary}

Supervised learning models are always present in the daily lives of data
scientists. Within these, linear regression models have represented a
powerful tool for understanding the relationship between phenomena and
predicting events, adding value by providing information for decision
making and business management. This paper aims to reinforce the
concepts of linear regression and sharpen the authors' skills as data
scientists, becoming an opportunity to strengthen concepts and put them
into practice for our future as professionals in the field. We will
cover simple linear regression models with and without interaction as
well as multiple regression models; we will find the results through
Python statistical packages as well as the matrix approach to parameter
calculations. Lastly, we wil conceptually discuss the logistic
regression prediction approach.

\textbf{Key terms}: Linear regression, multiple linear regression,
hypothesis tests, robust regression, interaction model, log-linear
regression.

    Iniciamos importando las librerías necesarias para la resolución de los
ejercicios propuestos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{228}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Importamos librerias necesarias}

\PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sn}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Scaler estandar}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}

\PY{c+c1}{\PYZsh{} Splitter para partir el dataset en entrenamiento/prueba}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{ejercicio-de-regresiuxf3n-lineal-con-interacciuxf3n}{%
\subsubsection{Ejercicio de regresión lineal con
interacción}\label{ejercicio-de-regresiuxf3n-lineal-con-interacciuxf3n}}

    Procedemos a importar nuestro primer set de datos, con el que
abordaremos el primer apartado de este documento.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{229}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Importar el data set}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}excel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D:}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{OneDrive \PYZhy{} Tecnoquimicas}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{99. PERSONAL}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Formación}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Maestria}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Semestre 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Analisis Cuantitivo}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Trabajo 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{data\PYZus{}exam1.xlsx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{sheet\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Continuamos con el análisis exploratorio, entendiendo la estructura del
data set, estadísticas descriptivas de las variables y adelantando el
análisis gráfico.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{230}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Se visualiza la estructura del dataset}
\PY{n}{df}\PY{o}{.}\PY{n}{shape}
\PY{n}{df}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1000 entries, 0 to 999
Data columns (total 3 columns):
 \#   Column  Non-Null Count  Dtype
---  ------  --------------  -----
 0   Y       1000 non-null   float64
 1   X       1000 non-null   float64
 2   Ind     1000 non-null   int64
dtypes: float64(2), int64(1)
memory usage: 23.6 KB
    \end{Verbatim}

    Se evidencia que no existen datos nulos para ninguan de las variables
del dataset. Además, existen dos variables que toman valores decimales
\textbf{``X''} \& \textbf{``Y''}, y la variable \textbf{``Ind''} que
toma valores enteros. Procedemos a calcular las estadísticas
descriptivas para el dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{231}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Se calculan las estadísticas }
\PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{231}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                 Y            X        Ind
count  1000.000000  1000.000000  1000.0000
mean     46.953751     9.976858     0.2000
std      22.046143     3.762567     0.4002
min     -34.894319    -4.263757     0.0000
25\%      32.427643     7.638899     0.0000
50\%      45.460252     9.952888     0.0000
75\%      61.587567    12.379984     0.0000
max     135.542574    25.628678     1.0000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{232}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Valores que puede tomar la variable Ind}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{232}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([0, 1], dtype=int64)
\end{Verbatim}
\end{tcolorbox}
        
    Nos damos cuenta que \textbf{``Ind''} es una variable categórica
binaria, es decir, toma valores entre 0 \& 1. Procedemos a graficar la
matriz de dispersión para revisar graficamente la relación entre las
variables.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{233}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sn}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{df}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{234}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Ordenación de los gráficos espacialmente en una matriz}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{sn}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot de la variable X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{sn}\PY{o}{.}\PY{n}{countplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de la variable Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Seteo del estilo de los gráficos}
\PY{n}{sn}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{235}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Gráfico de dispersión con linea de regresión}
\PY{n}{sn}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{data} \PY{o}{=} \PY{n}{df}\PY{p}{,} \PY{n}{x}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{line\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+m+mi}{7}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{palette}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mako}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{235}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<seaborn.axisgrid.FacetGrid at 0x14d927e9790>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al graficar las dos atributos, evidenciamos que para la variable
\textbf{`X'} existen un buen número de valores atípicos, tanto positivos
como negativos; sin embargo, no parece presentar un sesgo significativo
en la distribución de los datos, acercándose a una distribución normal.
Por otro lado, para la variable \textbf{`Ind'} vemos que es 4 veces el
número de registros que reportan 0 para esta variable que los que
reportan 1.

Al graficar la relación entre las variables evidenciamos una
distribución de las observaciones similar a una elipse, lo que
preliminarmente nos llevaría a creer que existe una relación lineal
positiva entre las variables, por lo podríamos plantear el uso de un
modelo de regresión lineal para explicar y predecir el comportamiento de
Y en función de X. podríamos

    Ahora, debido a que los posible valores que toma la variable
\textbf{`Ind'} son (0,1), asumimos que estamos lidiando con una variable
categórica de dos niveles, por lo que podríamos correr un modelo de
regresión múltiple, con la variable \textbf{`Ind'} como segunda variable
predictora, tal que se plantea el modelo

\[ \begin{align} 
y = \beta_0 + \beta_1X + \beta_2Ind + \epsilon = \left\{ 
    \begin{array}{ll}  \beta_0 + \beta_1X + \beta_2 + \epsilon & \text{si  Ind = 1} \\ 
    \beta_0 + \beta_1X + \epsilon & \text {si  Ind = 0} \end{array} \right. \end{align} \]

Así, podemos definir \(\beta_1\) como el cambio medio de la variable Y
por un cambio unitario de X, con todas las demás variables constantes;
igualmente, podemos definir \(\beta_2\) como a la agregación al valor
medio de Y cuando Ind toma el valor de 1.

Procedemos a correr el modelo:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{236}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{x} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}Apartar variables dependientes + constante}
\PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{c+c1}{\PYZsh{} Variable independiente }

\PY{c+c1}{\PYZsh{}Modelados}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{x}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Hacemos fit al modelo e imprimimos los resultados}
\PY{n}{results} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.759
Model:                            OLS   Adj. R-squared:                  0.758
Method:                 Least Squares   F-statistic:                     1566.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):          2.25e-308
Time:                        19:33:17   Log-Likelihood:                -3801.1
No. Observations:                1000   AIC:                             7608.
Df Residuals:                     997   BIC:                             7623.
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
X              4.9116      0.091     53.848      0.000       4.733       5.091
Ind          -14.1796      0.858    -16.535      0.000     -15.862     -12.497
const          0.7873      0.984      0.800      0.424      -1.143       2.718
==============================================================================
Omnibus:                        2.517   Durbin-Watson:                   1.997
Prob(Omnibus):                  0.284   Jarque-Bera (JB):                2.440
Skew:                           0.078   Prob(JB):                        0.295
Kurtosis:                       3.185   Cond. No.                         31.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    Al correr el OLS, nos encontramos con que el modelo lineal ajusta un
75.8\% los datos, con un p\_value \textless{} 0.05 que nos asegura la
significancia del modelo. Los resultados arrojados por el modelo nos
permite plantear el modelo

\[ \hat{Y} = \left\{ 
    \begin{array}{ll} (0.7873-14.1796) + 4.9116X & \text{si  Ind = 1} \\
    \ (0.7873) + 4.9116X & \text {si  Ind = 0} \end{array} \right. \]

Donde cada coeficiente lo podemos interpretar:

\[ \hat{\beta_{0}} = 0.7873 \text{ como intercepto. Es valor medio de } \hat{Y} \text{ cuando las demás variables toman valor de 0} \]
\[ \hat{\beta_{1}} = 4.9116 \text{ Ante un cambio en 1 unidad de la variable X, el valor esperado de Y aumentaría 4.9116 } \\ \text{dejando todas las demás variables constantes. Es significativo con un p-value < 0.05} \]
\[ \hat{\beta_{2}} = -14.1796. \text{ El valor medio de } \hat{Y} \text{ se reduce en -14.1769 si el individuo registra una valor de 1 en la variable Ind, independientemente del valor que tome X.}\\
\text{Es decir, para los individuos con Ind = 1, el valor esperado medio de Y será de -13.3923 si X tomará el valor de 0}\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{237}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Gráfico de dispersión con linea de regresión según el valor que tome Ind}

\PY{n}{sn}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{df}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{line\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+m+mi}{7}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{palette}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mako}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{237}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<seaborn.axisgrid.FacetGrid at 0x14d976db400>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al realizar el gráfico de dispersión diferenciando cada punto por la
variable \textbf{`Ind'} evidenciamos que para ambos casos (0,1) parece
existir una relación de dependencia positiva, sin embargo, los
individuos cuyo atributo Ind = 1 parecen reportar valores de Y menores
que los aquellos con Ind = 0, lo cual es coherente con lo visto en el
modelo anterior (con \(\beta_2 = -14.1796\)). Debido a que el modelo
anterior nos arrojó que la variable categórica es significativa para el
modelo, podemos pensar en correr un modelo con interacción entre las
variables \textbf{`X'} e \textbf{`Ind'}, que lo podemos plantear como:

\[ \begin{align} 
\hat{y} = \hat{\beta_0} + \hat{\beta_1}X + \hat{\beta_2}Ind + \hat{\beta_3}*X*Ind = \left\{ 
    \begin{array}{ll}  (\hat{\beta_0} +  \hat{\beta_2})+ (\hat{\beta_1} + \hat{\beta_3})X & \text{si  Ind = 1} \\ 
    \hat{\beta_0} + \hat{\beta_1}X & \text {si  Ind = 0} \end{array} \right. \end{align} \]

El planteamiento de este modelo nos muestra no solo un cambio en el
intercepto sino también un cambio en la pendiente del modelo, de manera
que nos permita probar si ante un cambio en 1 unidad de X, esta variable
termina afectando de manera distinta a los individuos dependiendo del
valor que tome la variable \textbf{`Ind'}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{238}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Generamos la variable interacción}
\PY{n}{interaccion} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{*}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Interaccion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{interaccion}

\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{238}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           Y          X  Ind  Interaccion
0  66.199147  12.653765    0     0.000000
1  44.311301   8.204418    0     0.000000
2  48.390783   8.768596    0     0.000000
3  58.087413  16.169568    1    16.169568
4  60.708671   9.980310    0     0.000000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{239}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Corremos el modelo con la interacción}
\PY{n}{x\PYZus{}inter} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Interaccion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}inter} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{}Modelamos}
\PY{n}{model\PYZus{}inter} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y\PYZus{}inter}\PY{p}{,}\PY{n}{x\PYZus{}inter}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Hacemos fit e imprimimos resultados}
\PY{n}{results\PYZus{}inter} \PY{o}{=} \PY{n}{model\PYZus{}inter}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results\PYZus{}inter}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.765
Model:                            OLS   Adj. R-squared:                  0.764
Method:                 Least Squares   F-statistic:                     1081.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):          1.34e-312
Time:                        19:33:18   Log-Likelihood:                -3787.5
No. Observations:                1000   AIC:                             7583.
Df Residuals:                     996   BIC:                             7603.
Df Model:                           3
Covariance Type:            nonrobust
===============================================================================
                  coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------
X               5.0411      0.093     53.997      0.000       4.858       5.224
Ind             4.5491      3.674      1.238      0.216      -2.661      11.759
Interaccion    -1.8466      0.353     -5.239      0.000      -2.538      -1.155
const          -0.4991      1.001     -0.498      0.618      -2.464       1.466
==============================================================================
Omnibus:                        4.301   Durbin-Watson:                   1.985
Prob(Omnibus):                  0.116   Jarque-Bera (JB):                4.811
Skew:                           0.065   Prob(JB):                       0.0902
Kurtosis:                       3.314   Cond. No.                         119.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    Al correr el modelo con la interacción evidenciamos que el ajuste del
modelo mejora, pasando de un 75.8\% a 76.4\% con un p\_value del modelo
menor a 0.05. Sin embargo, vemos que la variable de Ind que en el modelo
anterior era significativa deja de tener significancia y, en cambio, la
la interacción entre las dos variable cuenta con un
p\_value\textless0.05. Volvemos a correr el modelo sin la variable Ind y
revisamos sus resultados

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{240}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Corremos el modelo con la interacción eliminando la variable categórica Ind como predictor solo}
\PY{n}{x\PYZus{}inter2} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Interaccion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}inter2} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{model\PYZus{}inter2} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y\PYZus{}inter2}\PY{p}{,}\PY{n}{x\PYZus{}inter2}\PY{p}{)}

\PY{n}{results\PYZus{}inter2} \PY{o}{=} \PY{n}{model\PYZus{}inter2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results\PYZus{}inter2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.765
Model:                            OLS   Adj. R-squared:                  0.764
Method:                 Least Squares   F-statistic:                     1620.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):          6.34e-314
Time:                        19:33:18   Log-Likelihood:                -3788.2
No. Observations:                1000   AIC:                             7582.
Df Residuals:                     997   BIC:                             7597.
Df Model:                           2
Covariance Type:            nonrobust
===============================================================================
                  coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------
X               5.0120      0.090     55.465      0.000       4.835       5.189
Interaccion    -1.4219      0.081    -17.504      0.000      -1.581      -1.263
const          -0.1611      0.964     -0.167      0.867      -2.052       1.730
==============================================================================
Omnibus:                        3.980   Durbin-Watson:                   1.986
Prob(Omnibus):                  0.137   Jarque-Bera (JB):                4.433
Skew:                           0.057   Prob(JB):                        0.109
Kurtosis:                       3.306   Cond. No.                         31.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    Al correr nuevamente el modelo sin la variable \textbf{`Ind'} vemos que
no hay un cambio en el porcentaje de ajuste, sin embargo, nos deshacemos
de variables que no son significativas para el modelo. Por lo mismo, se
puede concluir que la interacción entre la variable \textbf{`Ind'} \&
\textbf{`X'} nos conduce generar un modelo que explica y predice de
mejor manera los valores de Y. Por tanto, planteamos el modelo e
interpretamos los coeficientes.

\[ \begin{align} 
\hat{y} = \hat{\beta_0} + \hat{\beta_1}X + \hat{\beta_3}*X*Ind = \left\{ 
    \begin{array}{ll} \hat{\beta_0}+ (\hat{\beta_1} + \hat{\beta_3})X & \text{si  Ind = 1} \\ 
    \hat{\beta_0} + \hat{\beta_1}X & \text {si  Ind = 0} \end{array} \right. \end{align} \]

\[ \begin{align} 
 = -0.1611 + 5.0120*X + -1.4219*X*Ind = \left\{ 
    \begin{array}{ll} -0.1611+ 3.5901*X & \text{si  Ind = 1} \\ 
    -0.1611 + 5.0120*X & \text {si  Ind = 0} \end{array} \right. \end{align} \]

    De esta manera, podemos concluir que ante un cambio unitario en X el
impacto medio en Y es distinto dependiendo del valor que tome la
variable \textbf{`Ind'}. Así,

\[ 
\hat{\beta_1} : 5.0120. \text{ y aumenta en promedio 5.0120 cuando X cambia en una unidad, dado si la variable Ind toma el valor de 0} \\
(\hat{\beta_1} + \hat{\beta_3}) : 3.5901. \text{ y aumenta en promedio 3.5901 cuando X cambia en una unidad, siempre que la variable Ind tome valor 1}
\]

Es decir, los individuos que reportan 1 en el atributo indicador
perciben un menor impacto ante cambios en X que aquellos que tiene 0
asignado. No hay cambios en el intercepto, pues la variable Ind por sí
sola no es significativa para el modelo.

    \hypertarget{prueba-de-supuestos-del-modelo}{%
\paragraph{Prueba de supuestos del
modelo}\label{prueba-de-supuestos-del-modelo}}

Una vez planteado el modelo, procedemos a validar cada uno de los
supuestos del modelo.

\[ \epsilon_i \overset{iid}\sim N(0, \sigma^2) \]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independencia de los errores
\end{enumerate}

\[ H_0: \rho(\epsilon_{i}, \epsilon_{i+1}) = 0 \\
H_1: \rho(\epsilon_{i}, \epsilon_{i+1}) \neq 0 \]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{241}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k+kn}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}

\PY{c+c1}{\PYZsh{} Test grafico de autocorrelaciones}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{y\PYZus{}inter2}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ACF de la variable Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}
\PY{n}{plot\PYZus{}pacf}\PY{p}{(}\PY{n}{y\PYZus{}inter2}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PACF de la variable Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al construir el grafico de autocorrelación simple y autocorrelación
parcial de la variable dependiente, aparentemente vemos que, con un lag
de 5 datos, solo el primero es significativo y nos llevaría a pensar que
existe independencia de los errores. Además, al correr el modelo nos
arroja un valor de Durbin - Watson de 1.986, muy cerca del 2 ideal, lo
que nos reforzaría la idea de no autocorrelación.

Corremos la prueba de Breusch - Godfrey definiendo el método y
presentando una tabla de validación de hipótesis, que se definen así:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{242}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{diagnostic} \PY{k+kn}{import} \PY{n}{acorr\PYZus{}breusch\PYZus{}godfrey}\PY{p}{,} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{,} \PY{n}{het\PYZus{}white}

\PY{c+c1}{\PYZsh{} Test Breusch Godfrey}
\PY{k}{def} \PY{n+nf}{test\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{model\PYZus{}results}\PY{p}{,} \PY{n}{maxlags}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{list} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{maxlags}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{values} \PY{o}{=} \PY{n}{acorr\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{model\PYZus{}results}\PY{p}{,} \PY{n}{nlags}\PY{o}{=}\PY{n}{i}\PY{p}{)}
        \PY{n+nb}{list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
    
    \PY{n}{table} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n+nb}{list}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lags}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{table}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lags}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} P\PYZhy{}value \PYZlt{} pvalue }
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.1}
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.05}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.05}
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.01}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.01}

    \PY{c+c1}{\PYZsh{} Rounding}
    \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
    \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

    \PY{k}{return} \PY{n}{table}


\PY{c+c1}{\PYZsh{} La hipotesis nula es de no autocorrelacion}
\PY{c+c1}{\PYZsh{} Se rechaza la no autocorrelacion hasta 3 rezagos. }
\PY{n}{test\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{results\PYZus{}inter2}\PY{p}{,} \PY{n}{maxlags}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{242}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
         LM  pvalue  pv<0.1  pv<0.05  pv<0.01
lags
1     0.050   0.823   False    False    False
2     1.864   0.394   False    False    False
3     3.396   0.335   False    False    False
\end{Verbatim}
\end{tcolorbox}
        
    Al correr el test de Breusch - Godfrey hasta en 3 rezagos podemos
aceptar la hipótesis nula y concluir que los errores son independiente,
por lo mismo no existe evidencia de \textbf{autocorrelación}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Continuamos con validación de la varianza constante de los errores.
\end{enumerate}

\[ H_0: \mathbb{V}(\epsilon_{i}) = Constante \\
H_1: \mathbb{V}(\epsilon_{i})  \neq Constante \]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{243}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test gráfico}
\PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{results\PYZus{}inter2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{x\PYZus{}inter2}\PY{p}{)}
\PY{n}{resid} \PY{o}{=} \PY{n}{results\PYZus{}inter2}\PY{o}{.}\PY{n}{resid}

\PY{n}{sn}\PY{o}{.}\PY{n}{scatterplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{resid}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Analisis grafico de los residuos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fuerza de compresión del concreto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuo}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{243}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0, 0.5, 'Residuo')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{244}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test Breusch \PYZhy{} Pagan}
\PY{n}{BP\PYZus{}test} \PY{o}{=} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{(}\PY{n}{resid}\PY{p}{,} \PY{n}{results\PYZus{}inter2}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{BP\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{BP\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El estadistico Breusch \PYZhy{} Pagan es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ y el p\PYZhy{}value es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Test de White}
\PY{n}{white\PYZus{}test} \PY{o}{=} \PY{n}{het\PYZus{}white}\PY{p}{(}\PY{n}{resid}\PY{p}{,} \PY{n}{results\PYZus{}inter2}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estadístico White}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Statistic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Test p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{white\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{white\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{labels}\PY{p}{,} \PY{n}{white\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El estadistico Breusch - Pagan es 80.02 y el p-value es 0.0
\{'Estadístico White': 83.151, 'p-value': 0.0, 'F-Statistic': 22.56, 'F-Test
p-value': 0.0\}
    \end{Verbatim}

    Ambas pruebas nos dan un p-value menor a 0.05, lo que nos lleva a
rechazar la hipótesis nula y concluir que el modelo presenta
heterocedasticidad, es decir, la varianza de los errores no es
constante.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Errores se distribuyen de manera normal. Adelantamos análisis gráfico
  y las pruebas de bondad de ajuste de D'Angostino y Jarque Bera.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{245}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{}Test gráfico de normalidad}
\PY{n}{sm}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{resid}\PY{p}{,} \PY{n}{line} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{246}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{normaltest}

\PY{c+c1}{\PYZsh{}Test 2 de bondad de ajuste}
\PY{n}{k2}\PY{p}{,} \PY{n}{p\PYZus{}value} \PY{o}{=} \PY{n}{normaltest}\PY{p}{(}\PY{n}{resid}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico = }\PY{l+s+si}{\PYZob{}}\PY{n}{k2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, p\PYZhy{}value = }\PY{l+s+si}{\PYZob{}}\PY{n}{p\PYZus{}value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico = 3.9803027284582315, p-value = 0.13667473621367196
    \end{Verbatim}

    Al graficar la tendencia de los errores, evidenciamos que este se acerca
mucho a la normalidad. Analizamos el p - valor del test de Jarque - Bera
y de la prueba de D'Angostino, los cuales están por encima del 0.05, lo
que conlleva a aceptar la hipótesis nula de normalidad de los residuos.

    \hypertarget{modelo-de-regresion-lineal-con-transformaciuxf3n-de-variables}{%
\subsubsection{Modelo de regresion lineal con transformación de
variables}\label{modelo-de-regresion-lineal-con-transformaciuxf3n-de-variables}}

    Para este numeral, se realiza un análisis univariante y bivariante al
conjunto de datos del fichero Data\_Exam1.xlsx. Posteriormente, se
realiza una transformación mediante la función logarítmica natural de la
Variable ``X'', con base en el comportamiento entre las variables ``X''
y ``Y'', observado mediante un acercamiento gráfico.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{247}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}lectura del Data set}
\PY{n}{df2} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}excel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D:}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{OneDrive \PYZhy{} Tecnoquimicas}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{99. PERSONAL}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Formación}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Maestria}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Semestre 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Analisis Cuantitivo}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Trabajo 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{data\PYZus{}exam1.xlsx}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sheet\PYZus{}name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{df2}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{247}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           Y         X
0  12.189142  0.226957
1  12.187456  0.088938
2  11.782692  0.199069
3   5.732032  0.003812
4   7.026970  0.004573
\end{Verbatim}
\end{tcolorbox}
        
    En un primer acercamiento univariante a los datos mediante la función
``describe'' de Pandas, y complementándolo con una revisión gráfica por
medio de diagramas de cajas para el comportamiento de los datos de las
variables ``X'' y ``Y'', se resaltan los siguientes puntos:

\emph{Variable X} Los valores de la variable X, presentan un valor medio
de 0.07234, con una desviación estándar de 0.09753.

\emph{El primer cuartil (25\%) es aproximadamente 0.0085. }La mediana de
X (50\%) es aproximadamente 0.0366. \emph{El tercer cuartil (75\%) de
los valores de X es aproximadamente 0.0999. }El valor máximo de ``X'' es
aproximadamente 0.9397, lo que sugiere que hay valores muy grandes
presentes en los datos.

Mediante los datos anteriores y complementando con la información
gráfica presentada en los sigientes gráficos, se resalta para los
valores de X que: presentan una distribución sesgada hacia la derecha,
que la media (0.07234) es significativamente mayor que la mediana
(0.0366) y que hay presencia de valores atípicos considerablemente
superiores en el último cuartil.

\emph{Variable Y} Los valores de la variable ``Y'', presentan un valor
medio de 9.4456, con una desviación estándar de 3.90818.

El 25\% de los datos de Y están por debajo de aproximadamente 7.41, el
50\% de los datos de Y están por debajo de aproximadamente 10.07, y el
75\% de los datos de ``Y'' están por debajo de aproximadamente 12.08.

Mediante los datos anteriores y complementando con la información
gráfica presentada en los sigientes gráficos, se resalta para los
valores de Y que: presentan una distribución sesgada hacia la izquierda,
se encuentran valores atípicos significativamente menores en el primer
cuartil.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{248}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df2}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{248}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                 Y             X
count  1000.000000  1.000000e+03
mean      9.445622  7.234805e-02
std       3.908189  9.753985e-02
min     -12.073239  1.343729e-08
25\%       7.411486  8.450417e-03
50\%      10.072134  3.655172e-02
75\%      12.082546  9.992523e-02
max      17.838788  9.397465e-01
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{249}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Ordenación de los gráficos espacialmente en una matriz}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{sn}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df2}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot de la variable X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{sn}\PY{o}{.}\PY{n}{histplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df2}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de la variable Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Seteo del estilo de los gráficos}
\PY{n}{sn}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{250}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Ordenación de los gráficos espacialmente en una matriz}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{sn}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df2}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot de la variable X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\PY{n}{sn}\PY{o}{.}\PY{n}{histplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{df2}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de la variable Ind}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Seteo del estilo de los gráficos}
\PY{n}{sn}\PY{o}{.}\PY{n}{set\PYZus{}style}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkgrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Continuamos con el análisis bivariante:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{251}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sn}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{df2}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{252}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} calculando la correlación de pearson}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== Pearson =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df2}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pearson}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== spearman =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} calculando la correlación de spearman}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df2}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spearman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== kendall =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} calculando la correlación de kendall}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df2}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kendall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
============== Pearson =============
          Y         X
Y  1.000000  0.520376
X  0.520376  1.000000
============== spearman =============
          Y         X
Y  1.000000  0.767833
X  0.767833  1.000000
============== kendall =============
          Y         X
Y  1.000000  0.580777
X  0.580777  1.000000
    \end{Verbatim}

    Se calculan las matrices de correlación, buscando información sobre la
relación entre las variables X y Y.

*\textbf{Correlación de Pearson}: Se obtiene una correlación de 0.52,
sugiriendo una relación moderada positiva entre ``X'' y ``Y''.

*\textbf{Correlación de Spearman}: Se obtiene una correlación 0.77,
sugiriendo una correlación fuerte y positiva entre ``X'' y ``Y''.

*\textbf{Correlación de Kendall}: Se obtiene una correlación 0.58,
sugiriendo una correlación moderada y positiva entre ``X'' y ``Y''.

Los resultados indican una correlación positiva entre las variables Y y
X. La fuerza de esta correlación varía ligeramente según el método
utilizado, pero en general, sugiere que existe una relación positiva
entre las dos variables

Continuamos con la transformación de la variable independiente para la
corrida del modelo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{253}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Aplicar la transformación logarítmica a la variable X}
\PY{n}{df2}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{df2}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{254}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{X} \PY{o}{=} \PY{n}{df2}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}log}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}  \PY{c+c1}{\PYZsh{} Variable explicativa}
\PY{n}{y} \PY{o}{=} \PY{n}{df2}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}        \PY{c+c1}{\PYZsh{} Variable objetivo}

\PY{c+c1}{\PYZsh{} Agregar intercepto al conjunto de datos}
\PY{n}{X} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir los resultados del modelo}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.732
Model:                            OLS   Adj. R-squared:                  0.732
Method:                 Least Squares   F-statistic:                     2726.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):          1.29e-287
Time:                        19:33:20   Log-Likelihood:                -2123.1
No. Observations:                1000   AIC:                             4250.
Df Residuals:                     998   BIC:                             4260.
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         15.1706      0.127    119.481      0.000      14.921      15.420
X\_log          1.4987      0.029     52.209      0.000       1.442       1.555
==============================================================================
Omnibus:                        0.330   Durbin-Watson:                   1.982
Prob(Omnibus):                  0.848   Jarque-Bera (JB):                0.305
Skew:                           0.043   Prob(JB):                        0.858
Kurtosis:                       3.005   Cond. No.                         9.11
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    Se aplica una transformación logarítmica a la variable `X' utilizando la
función logarítmica natural. Posterior a esta transformación, se procede
a ajustar un modelo de regresión lineal mediante el método de mínimos
cuadrados ordinarios (OLS), utilizando la variable transformada `X\_log'
como variable explicativa y la variable `Y' como variable objetivo.

De los valores obtenidos, se obtiene que la siguiente, es la función
estimada que describe la relación entre la variable de respuesta Y y la
variable explicativa X, según el modelo de regresión lineal ajustado.

\(\hat{Y} =β₀ +β₁ ⋅Xlog​\)

Donde:

\(\hat{Y}\) es la variable de respuesta (Y) estimada.

β₀ es el coeficiente de intersección (constante).

β₁ es el coeficiente asociado con la variable explicativa X\_log

Encontrando que la siguiente es la función estimada de Y en función de X

\[ \hat{Y}=15.1706+1.4987⋅ln(X) \]

    Una vez corrido el modelo, intepretamos los estadísticos que nos arroja:

\textbf{Coeficiente (coef)}: El coeficiente para la variable X\_log es
1.4987. Esto sugiere que, en promedio, un aumento unitario en X\_log
está asociado con un aumento de aproximadamente 1.4987 unidades en Y.

\textbf{Valores p (P\textgreater\textbar t\textbar)}: El valor p
obtenido en el modelo, para el coeficiente de X\_log es 0.000, lo que
sugiere que el efecto de X\_log en Y es estadísticamente significativo.

\textbf{R-cuadrado}: El modelo arroja un valor de R-cuadrado de 0.732,
indicando que aproximadamente el 73.2\% de la variabilidad en la
variable de respuesta (Y) es explicada por la variable explicativa
(X\_log).

\textbf{F-statistic} y \textbf{valor p asociado} : Se obtiene un valor p
asociado significativamente bajo (1.29e-287), indicando que el modelo es
estadísticamente significativo.

    \hypertarget{validaciuxf3n-de-los-supuestos-del-modelo}{%
\paragraph{Validación de los supuestos del
modelo}\label{validaciuxf3n-de-los-supuestos-del-modelo}}

A continuación, se presenta la evaluación de los supuestos del modelo,
encontrando que no hay evidencia para rechazar ningun de los supuestos
frente a los residuales.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{255}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{residuals} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{resid}
\end{Verbatim}
\end{tcolorbox}

    Supuesto 1 y supuesto 3 y 4: Los residuales son independientes, Promedio
de los 𝓔ᵢ= 0 y presentan varianza constante

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{256}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{predictions}\PY{p}{,} \PY{n}{residuals}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gráfico de Dispersión de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicciones}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_57_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    De acuerdo con el gráfico anterior, se encuentra que los residuos, no
muestran un comportamiento constante al rededor del cero,
especificamente, para valores mayores en el eje X, se observan mayor
valores de residuo, pareciera que los residuos están indicando
heterocedasticidad, es decir, que la varianza de los errores no es
constante a lo largo del rango de predicciones.

A continuación se proceder a evaluar mediante tests de Breusch-Pagan y
Durbin-Watson, si el comportamiento de los residuos presentan o no,
heterocedasticidad y autocorrelación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{257}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{diagnostic} \PY{k+kn}{import} \PY{n}{het\PYZus{}breuschpagan}

\PY{c+c1}{\PYZsh{} Calcula el test de Breusch\PYZhy{}Pagan}
\PY{n}{lm}\PY{p}{,} \PY{n}{lm\PYZus{}p\PYZus{}value}\PY{p}{,} \PY{n}{fvalue}\PY{p}{,} \PY{n}{f\PYZus{}p\PYZus{}value} \PY{o}{=} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{(}\PY{n}{residuals}\PY{p}{,} \PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprime los resultados}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM Estadístico:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lm}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor LM:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lm\PYZus{}p\PYZus{}value}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F Estadístico:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{fvalue}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor F:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{f\PYZus{}p\PYZus{}value}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
LM Estadístico: 0.5282662931659354
P-valor LM: 0.46733665913366673
F Estadístico: 0.5274884149292246
P-valor F: 0.46783499688166175
    \end{Verbatim}

    Los resultados del test de Breusch-Pagan muestran que el estadístico LM
es 0.528 y el valor p asociado es 0.467 para el estadístico LM. Además,
el estadístico F es 0.527 con un valor p asociado de 0.468.

Dado que los valores p para ambos estadísticos, son mayores que 0.05, no
hay suficiente evidencia para rechazar la hipótesis nula de
homocedasticidad en los residuos. Los residuos parecen tener una
varianza constante, lo que sugiere que no hay heterocedasticidad en el
modelo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{258}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k+kn}{import} \PY{n}{durbin\PYZus{}watson}

\PY{n}{residuos} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{resid}

\PY{c+c1}{\PYZsh{} Calcular el estadístico de Durbin\PYZhy{}Watson}
\PY{n}{statistic} \PY{o}{=} \PY{n}{durbin\PYZus{}watson}\PY{p}{(}\PY{n}{residuos}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Durbin\PYZhy{}Watson:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{statistic}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Durbin-Watson: 1.9819250655292384
    \end{Verbatim}

    Dado que el valor obtenido para el estadístico de Durbin-Watson es
aproximadamente 1.98, está cerca de 2, pareciera que no hay
autocorrelación de primer orden en los residuos. En consecuencia, los
residuos parecen ser independientes entre sí.

Supuesto 2: Los 𝓔ᵢ presentan una distribución Normal

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{259}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{residuals}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frecuencia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{260}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test de Shapiro\PYZhy{}Wilk para validación de Normalidad en los residuales}

\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}

\PY{n}{shapiro\PYZus{}result} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{shapiro}\PY{p}{(}\PY{n}{residuals}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado del test}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Shapiro\PYZhy{}Wilk:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Interpretar el resultado del test}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{k}{if} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue} \PY{o}{\PYZgt{}} \PY{n}{alpha}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se puede rechazar la hipótesis nula. Los residuos siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Shapiro-Wilk: 0.9991617550230457
P-valor: 0.9427671920709753
No se puede rechazar la hipótesis nula. Los residuos siguen una distribución
normal.
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{261}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}import statsmodels.api as sm}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{gofplots} \PY{k}{as} \PY{n+nn}{smg}

\PY{c+c1}{\PYZsh{}  gráfico QQPlot}
\PY{n}{fig} \PY{o}{=} \PY{n}{smg}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{residuals}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Mostrar el gráfico}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_65_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Considerando el acercamiento gráfico de los residuales del modelo,
mediante el histograma de los residuales y el diagrama QQPlot, se pueden
apreciar comportamientos de los residuales que se aproximan a una
distribución Normal. se realizó el test de Shapiro-Wilk, obteniendo un
valor p de 0.943. Dado que este valor p es superior al nivel de
significancia establecido, no se puede rechazar la hipótesis nula de
normalidad. Por lo tanto, se concluye que los residuos se distribuyen de
manera normal

    \hypertarget{ejercicio-de-regresiuxf3n-lineal-con-cuxe1lculo-matricial}{%
\subsubsection{Ejercicio de regresión lineal con cálculo
matricial}\label{ejercicio-de-regresiuxf3n-lineal-con-cuxe1lculo-matricial}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{262}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Se excluyen dos filas ya que la primera fila se trata del título, y segunda fila es un espacio en blanco}
\PY{n}{df3} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}excel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D:}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{OneDrive \PYZhy{} Tecnoquimicas}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{99. PERSONAL}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Formación}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Maestria}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Semestre 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Analisis Cuantitivo}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Trabajo 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{datos.xls}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{sheet\PYZus{}name}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Wine Quality}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{skiprows}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{df3}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{262}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Calidad del Vino  Acidez Fija  Acidez Volátil  Ácido Cítrico  \textbackslash{}
0                 6          7.0            0.27           0.36
1                 6          6.3            0.30           0.34
2                 6          8.1            0.28           0.40
3                 6          7.2            0.23           0.32
4                 6          7.2            0.23           0.32

   Azúcar Residual  Cloruros  Dióxido de Azúfre Libre  \textbackslash{}
0             20.7     0.045                     45.0
1              1.6     0.049                     14.0
2              6.9     0.050                     30.0
3              8.5     0.058                     47.0
4              8.5     0.058                     47.0

   Dióxido de Azúfre Total  Densidad    pH  Sulfatos  Alcohol
0                    170.0    1.0010  3.00      0.45      8.8
1                    132.0    0.9940  3.30      0.49      9.5
2                     97.0    0.9951  3.26      0.44     10.1
3                    186.0    0.9956  3.19      0.40      9.9
4                    186.0    0.9956  3.19      0.40      9.9
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{263}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df3}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4898 entries, 0 to 4897
Data columns (total 12 columns):
 \#   Column                   Non-Null Count  Dtype
---  ------                   --------------  -----
 0   Calidad del Vino         4898 non-null   int64
 1   Acidez Fija              4898 non-null   float64
 2   Acidez Volátil           4898 non-null   float64
 3   Ácido Cítrico            4898 non-null   float64
 4   Azúcar Residual          4898 non-null   float64
 5   Cloruros                 4898 non-null   float64
 6   Dióxido de Azúfre Libre  4898 non-null   float64
 7   Dióxido de Azúfre Total  4898 non-null   float64
 8   Densidad                 4898 non-null   float64
 9   pH                       4898 non-null   float64
 10  Sulfatos                 4898 non-null   float64
 11  Alcohol                  4898 non-null   float64
dtypes: float64(11), int64(1)
memory usage: 459.3 KB
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{264}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{columnas\PYZus{}a\PYZus{}excluir} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pH}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sulfatos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cloruros}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Acidez Volátil}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Acidez Fija}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Calidad del Vino}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Crear un nuevo DataFrame excluyendo las columnas especificadas}
\PY{n}{df3} \PY{o}{=} \PY{n}{df3}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{n}{columnas\PYZus{}a\PYZus{}excluir}\PY{p}{)}

\PY{n}{columnas\PYZus{}ordenadas} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+} \PY{p}{[}\PY{n}{col} \PY{k}{for} \PY{n}{col} \PY{o+ow}{in} \PY{n}{df3}\PY{o}{.}\PY{n}{columns} \PY{k}{if} \PY{n}{col} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{df3} \PY{o}{=} \PY{n}{df3}\PY{p}{[}\PY{n}{columnas\PYZus{}ordenadas}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{265}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sn}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{df3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{265}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<seaborn.axisgrid.PairGrid at 0x14d921ea910>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_71_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{266}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{df3}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{df3}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{267}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{267}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[0.99272],
       [0.9968 ],
       [0.99268],
       {\ldots},
       [0.99129],
       [0.99567],
       [0.99255]])
\end{Verbatim}
\end{tcolorbox}
        
    Estandarizamos las variables, lo que nos garantiza que la matriz de
covarianza se calculará estandarizada, es decir, calcularemos los
indices de correlación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{268}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Estandarización de las variables}
\PY{n}{scalerX} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Esta linea instancia la clase que va a calcular la estandarización}
\PY{n}{scalerX}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Ajustamos el scaler a nuestros datos df}
\PY{n}{datosX\PYZus{}scaled} \PY{o}{=} \PY{n}{scalerX}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Así calculo la estandarización de los datos}
\PY{n}{dfX\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{datosX\PYZus{}scaled}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{dfX\PYZus{}scaled}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{268}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Ácido Cítrico  Azúcar Residual  Dióxido de Azúfre Libre  \textbackslash{}
0       0.227731         0.340419                 0.534065
1       0.895832         1.002071                 0.773947
2      -0.022807         0.184737                -0.605377
3       0.144218        -0.924503                -0.125612
4      -0.607396         2.432407                 0.054300

   Dióxido de Azúfre Total   Alcohol
0                -0.641932  1.540371
1                 1.355106 -0.821712
2                -1.022320  0.481506
3                -0.879675  0.237153
4                 0.855846 -0.088652
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{269}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{scalerY} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
\PY{n}{scalerY}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{datosY\PYZus{}scaled} \PY{o}{=} \PY{n}{scalerY}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Así calculo la estandarización de los datos}
\PY{n}{datosY\PYZus{}scaled}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{269}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[-0.44704073],
       [ 0.90336975],
       [-0.46028004],
       {\ldots},
       [-0.92034636],
       [ 0.52935901],
       [-0.50330783]])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{270}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Mediante la matriz de covarianzas, se valida que los datos están estandazarizdos, se obtienen 1 en la diagonal}
\PY{n}{dfX\PYZus{}scaled}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{270}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                         Ácido Cítrico  Azúcar Residual  \textbackslash{}
Ácido Cítrico                 1.000255         0.092536
Azúcar Residual               0.092536         1.000255
Dióxido de Azúfre Libre       0.098112         0.306096
Dióxido de Azúfre Total       0.122396         0.406054
Alcohol                      -0.062996        -0.453598

                         Dióxido de Azúfre Libre  Dióxido de Azúfre Total  \textbackslash{}
Ácido Cítrico                           0.098112                 0.122396
Azúcar Residual                         0.306096                 0.406054
Dióxido de Azúfre Libre                 1.000255                 0.614348
Dióxido de Azúfre Total                 0.614348                 1.000255
Alcohol                                -0.257323                -0.454558

                          Alcohol
Ácido Cítrico           -0.062996
Azúcar Residual         -0.453598
Dióxido de Azúfre Libre -0.257323
Dióxido de Azúfre Total -0.454558
Alcohol                  1.000255
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{271}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{datosY\PYZus{}scaled}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{271}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
array([[-0.44704073,  0.90336975, -0.46028004, {\ldots}, -0.92034636,
         0.52935901, -0.50330783]])
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{272}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Se ubica a la variable Densidad como primer Variable de izquierda a derecha dentro del conjunto del dataset}
\PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{dfX\PYZus{}scaled}
\PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{datosY\PYZus{}scaled}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

\PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ácido Cítrico}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Azúcar Residual}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dióxido de Azúfre Libre}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dióxido de Azúfre Total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcohol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Cambiar los nombres de las columnas}
\PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{rename}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dióxido de Azúfre Libre}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DióxidoAL}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ácido Cítrico}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ÁcidoC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Azúcar Residual}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AzúcarR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                                      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dióxido de Azúfre Total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DióxidoZT}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Reordenar las columnas}
\PY{n}{df\PYZus{}scaled} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ÁcidoC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AzúcarR}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DióxidoAL}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DióxidoZT}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcohol}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}

\PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{272}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Densidad    ÁcidoC   AzúcarR  DióxidoAL  DióxidoZT   Alcohol
0 -0.447041  0.227731  0.340419   0.534065  -0.641932  1.540371
1  0.903370  0.895832  1.002071   0.773947   1.355106 -0.821712
2 -0.460280 -0.022807  0.184737  -0.605377  -1.022320  0.481506
3 -0.304718  0.144218 -0.924503  -0.125612  -0.879675  0.237153
4  1.883079 -0.607396  2.432407   0.054300   0.855846 -0.088652
\end{Verbatim}
\end{tcolorbox}
        
    Una vez tenemos las variables estandarizadas, procedemos a hacer el
cálculo de coeficientes a través de la solución matricial.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{273}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{} calculando la correlación de pearson}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== Pearson =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pearson}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Redondear a 2 cifras decimales}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== spearman =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} calculando la correlación de spearman}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spearman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Redondear a 2 cifras decimales}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{============== kendall =============}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} calculando la correlación de kendall}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kendall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Redondear a 2 cifras decimales}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
============== Pearson =============
           Densidad  ÁcidoC  AzúcarR  DióxidoAL  DióxidoZT  Alcohol
Densidad      1.000   0.143    0.843      0.300      0.532   -0.775
ÁcidoC        0.143   1.000    0.093      0.098      0.122   -0.063
AzúcarR       0.843   0.093    1.000      0.306      0.406   -0.453
DióxidoAL     0.300   0.098    0.306      1.000      0.614   -0.257
DióxidoZT     0.532   0.122    0.406      0.614      1.000   -0.454
Alcohol      -0.775  -0.063   -0.453     -0.257     -0.454    1.000
============== spearman =============
           Densidad  ÁcidoC  AzúcarR  DióxidoAL  DióxidoZT  Alcohol
Densidad      1.000   0.088    0.781      0.329      0.565   -0.823
ÁcidoC        0.088   1.000    0.026      0.092      0.094   -0.021
AzúcarR       0.781   0.026    1.000      0.345      0.429   -0.449
DióxidoAL     0.329   0.092    0.345      1.000      0.622   -0.275
DióxidoZT     0.565   0.094    0.429      0.622      1.000   -0.479
Alcohol      -0.823  -0.021   -0.449     -0.275     -0.479    1.000
============== kendall =============
           Densidad  ÁcidoC  AzúcarR  DióxidoAL  DióxidoZT  Alcohol
Densidad      1.000   0.059    0.590      0.218      0.389   -0.636
ÁcidoC        0.059   1.000    0.016      0.063      0.063   -0.014
AzúcarR       0.590   0.016    1.000      0.236      0.292   -0.308
DióxidoAL     0.218   0.063    0.236      1.000      0.448   -0.184
DióxidoZT     0.389   0.063    0.292      0.448      1.000   -0.327
Alcohol      -0.636  -0.014   -0.308     -0.184     -0.327    1.000
    \end{Verbatim}

    Considerando los valores obtenidos en las matrices de correlación, se
resaltan las siguientes observaciones sobre las estructuras de
dependencia entre las variables:

\textbf{\emph{En la matriz de correlación de Pearson:}} *La densidad
parece estar altamente correlacionada con el azúcar residual (0.843) y
moderadamente correlacionada con el dióxido de azufre libre (0.300) y el
dióxido de azufre total (0.532). Además, muestra una correlación
negativa fuerte con el alcohol (-0.775).

*El alcohol parece estar negativamente correlacionado con la densidad
(-0.775). Las otras correlaciones son relativamente bajas.

\textbf{\emph{En la matriz de correlación de Spearman:}} Se observan
patrones similares a la matriz de correlación de Pearson, pero las
correlaciones tienden a ser ligeramente menores.

La densidad aún muestra una alta correlación con el azúcar residual
(0.781) y correlaciones negativas fuertes con el alcohol (-0.823).

\textbf{\emph{En la matriz de correlación de Kendall:}} Los patrones
generales son similares a las matrices anteriores, pero las
correlaciones tienden a ser más bajas.

En resumen, las estructuras de dependencia observadas en la

    \hypertarget{modelo-de-regresiuxf3n-lineal}{%
\paragraph{Modelo de regresión
lineal}\label{modelo-de-regresiuxf3n-lineal}}

Dado que los datos del modelo de regresión se encuentran estandarizado,
se procede a calcular los coeficientes regresión directamente con la
matrix de correlaciones \(C\) de la sigueinte forma

\[\beta_{1\ldots p}=C_{XX}^{-1}C_Xy \hspace{1 cm} \beta_0=0\]

Partiendo de los datos estandarizados, y tomando como variable respuesta
Columna Densidad, se han construído 3 modelos RLM partiendo de las
matrices de correlación calculadas mediante los métodos Pearson, Kendall
y Spearman.

Para los 3 modelos se calcula el RMSE de la predicción, obteniendo los
siguientes valores para cada modelo, encontrado que el Modelo calculado
a partir de la matriz de correlación de kendall, reduce las desviaciones
de las predicción del modelo, respecto a los valores reales, comparados
con los otros dos modelos RLM calculados.

RMSE (Pearson)= 1.3863748432263951 RMSE (Kendall)= 1.2968506359111867
RMSE (Spearman)= 1.3875694897018205

Ninguno de los tres modelos de RLM calculados, cumplen los supuestos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{274}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}A continuación se calculan los coeficientes de regresión mediante la expresión anterior de matriz de correlación}

\PY{c+c1}{\PYZsh{}Definimos una función que permita hacer el cálculo de los coeficientes mediante diferentes métodos}

\PY{k}{def} \PY{n+nf}{RLC}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{p}{,} \PY{n}{method\PYZus{}name}\PY{p}{)}\PY{p}{:}

  \PY{n}{C} \PY{o}{=} \PY{n}{df\PYZus{}scaled}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{n}{method} \PY{o}{=} \PY{n}{method\PYZus{}name}\PY{p}{)}

  \PY{n}{CXX} \PY{o}{=} \PY{n}{C}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{}Definición de la matriz CXX No considera la primera columna ni la primera fila, la cual hace referencia a la variable \PYZdq{}Densidad\PYZdq{} que es la variable respuesta}

  \PY{n}{CXy} \PY{o}{=} \PY{n}{C}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{}Definición del Vecto CXY Vector de correlación de las variables de entrada respecto a la variable respuesta}

  \PY{n}{betas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{CXX}\PY{p}{)}\PY{p}{,} \PY{n}{CXy}\PY{p}{)} \PY{c+c1}{\PYZsh{}Cálculo de los Betas}

  \PY{k}{return} \PY{n}{betas}
\end{Verbatim}
\end{tcolorbox}

    Calculamos los coeficientes del modelo usando Spearman

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{275}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}A continuación se calculan los coeficientes de regresión mediante la expresión anterior de matriz de correlación}
\PY{c+c1}{\PYZsh{}Método Spearman}
\PY{c+c1}{\PYZsh{}B0=0 considerando que los datos están estandarizados}

\PY{n}{betas} \PY{o}{=} \PY{n}{RLC}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{spearman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{betas}
\PY{c+c1}{\PYZsh{}Los valores de Beta mayores, representan mayor dependencia a la variable}

\PY{c+c1}{\PYZsh{} Imprimir el mensaje en pantalla}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La siguiente es la expresión del modelo calculado, estimando los coeficientes de regresión mediante la matriz de correlación obtenida usando Spearman, }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y= }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₁}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₂}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₃}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₄}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₅}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
La siguiente es la expresión del modelo calculado, estimando los coeficientes de
regresión mediante la matriz de correlación obtenida usando Spearman,
y= β₁0.058673439357240245 + β₂0.5025917375311488 + β₃-0.08268649813250786 +
β₄0.12801828839822846 + β₅-0.557513820380729
    \end{Verbatim}

    Siendo β₁ asociado a los valores obtenidos por la variable Ácido Cítrico
β₂ asociado a los valores obtenidos por la variable Azúcar Residual β₃
asociado a los valores obtenidos por la variable Dióxido de Azúfre Libre
β₄ asociado a los valores obtenidos por la variable Dióxido de Azúfre
Total β₅ asociado a los valores obtenidos por la variable Alcohol

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{276}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scalerX}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{betas}\PY{p}{)} \PY{c+c1}{\PYZsh{}}

\PY{n}{residuales} \PY{o}{=} \PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{scalerY}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{277}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}

\PY{c+c1}{\PYZsh{} Calcular el RMSE}
\PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{En promedio, las predicciones del modelo tienen un error de aproximadamente:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{rmse}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
En promedio, las predicciones del modelo tienen un error de aproximadamente:
1.3875694897018205
    \end{Verbatim}

    Considerando que las variables del modelo, están estandarizadas, el RMSE
obtenido, indica que en promedio, las predicciones del modelo mediante
el método Spearman, están desviadas de los valores reales en alrededor
de 1.3876 desviaciones estándar de la variable objetivo (Densidad).

    \hypertarget{validaciuxf3n-de-supuestos-spearman}{%
\paragraph{Validación de supuestos
(Spearman)}\label{validaciuxf3n-de-supuestos-spearman}}

Mediante la evaluación de los supuestos del modelo, presentada a
continuación, se encuentra que hay evidencia significativa para rechazar
la hipótesis nula de homocedasticidad. Por lo tanto, se concluye que hay
heterocedasticidad en los residuos del modelo, adicionalmente, se
rechaza la hipótesis nula en el test Shapiro-Wilk; Los residuos no
siguen una distribución normal.

En resumen, dado que se encuentra evidencia de heterocedasticidad y no
se cumple el supuesto de normalidad de los residuos, no es correcto el
uso del modelo y tendría que revisarse técnicas de modelado alternativas
o ajustes en el modelo para abordar estas deficiencias y mejorar la
precisión de las estimaciones.

Supuesto 1 y supuesto 3 y 4: Los residuales son independientes, Promedio
de los 𝓔ᵢ= 0 y presentan varianza constante

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{278}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{residuales}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Gráfico de Dispersión de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicciones}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agregar línea horizontal en y=0}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_92_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    De acuerdo con el gráfico anterior, se encuentra que los residuos,
parecieran mostrar un comportamiento constante al rededor de cero, sin
embargo pareciera mostrar una leve concentración en valores superiores a
cero.

A continuación se proceder a evaluar mediante tests de Breusch-Pagan y
Durbin-Watson, si el comportamiento de los residuos presentan o no,
heterocedasticidad y autocorrelación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{279}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajusta un modelo auxiliar para explicar la varianza de los residuos}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{X\PYZus{}test}  \PY{c+c1}{\PYZsh{} Puedes utilizar las variables originales o alguna transformación de estas}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X\PYZus{}auxiliar}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agrega una constante si es necesario}
\PY{n}{model\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}auxiliar}\PY{p}{)}
\PY{n}{results\PYZus{}auxiliar} \PY{o}{=} \PY{n}{model\PYZus{}auxiliar}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Calcula el estadístico LM para el test de Breusch\PYZhy{}Pagan}
\PY{n}{BP\PYZus{}statistic} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{rsquared}
\PY{c+c1}{\PYZsh{} Calcula el p\PYZhy{}valor asociado al estadístico LM}
\PY{n}{p\PYZus{}valor\PYZus{}BP} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{f\PYZus{}pvalue}

\PY{c+c1}{\PYZsh{} Imprime los resultados}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{BP\PYZus{}statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor del test de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{p\PYZus{}valor\PYZus{}BP}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Breusch-Pagan: 0.07871791973180509
P-valor del test de Breusch-Pagan: 8.475640489544816e-16
    \end{Verbatim}

    Teniendo en cuenta los resultados del test de Breusch-Pagan, dado un
valor p significativamente bajo, hay evidencia significativa para
rechazar la hipótesis nula de homocedasticidad. Por lo tanto, se
concluye que hay heterocedasticidad en los residuos del modelo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{280}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k+kn}{import} \PY{n}{durbin\PYZus{}watson}

\PY{n}{residuos} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{resid}

\PY{c+c1}{\PYZsh{} Calcular el estadístico de Durbin\PYZhy{}Watson}
\PY{n}{statistic} \PY{o}{=} \PY{n}{durbin\PYZus{}watson}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Durbin\PYZhy{}Watson:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{statistic}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Durbin-Watson: 2.0374809484643865
    \end{Verbatim}

    Dado que el valor obtenido para el estadístico de Durbin-Watson es
aproximadamente 2.0375, está cerca de 2, pareciera que no hay
autocorrelación de primer orden en los residuos. En consecuencia, los
residuos parecen ser independientes entre sí.

Supuesto 2: Los 𝓔ᵢ presentan una distribución Normal

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{281}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frecuencia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_98_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{282}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}import statsmodels.api as sm}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{gofplots} \PY{k}{as} \PY{n+nn}{smg}

\PY{c+c1}{\PYZsh{}  gráfico QQPlot}
\PY{n}{fig} \PY{o}{=} \PY{n}{smg}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Mostrar el gráfico}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{283}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test de Shapiro\PYZhy{}Wilk para validación de Normalidad en los residuales}

\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}

\PY{n}{shapiro\PYZus{}result} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{shapiro}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado del test}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Shapiro\PYZhy{}Wilk:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Interpretar el resultado del test}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{k}{if} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue} \PY{o}{\PYZgt{}} \PY{n}{alpha}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se puede rechazar la hipótesis nula. Los residuos siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Shapiro-Wilk: 0.9878932480248609
P-valor: 3.1071658287470245e-07
Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.
    \end{Verbatim}

    Cálculo del modelo de regresion usando Kendall:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{284}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}A continuación se calculan los coeficientes de regresión mediante la expresión anterior de matriz de correlación}
\PY{c+c1}{\PYZsh{}Método Kendall}
\PY{c+c1}{\PYZsh{}B0=0 considerando que los datos están estandarizados}

\PY{n}{betas} \PY{o}{=} \PY{n}{RLC}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kendall}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{betas}
\PY{c+c1}{\PYZsh{}Los valores de Beta mayores, representan mayor dependencia a la variable}

\PY{c+c1}{\PYZsh{} Imprimir el mensaje en pantalla}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La siguiente es la expresión del modelo calculado, estimando los coeficientes de regresión mediante la matriz de correlación obtenida usando Spearman, }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y= }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₁}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₂}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₃}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₄}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₅}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
La siguiente es la expresión del modelo calculado, estimando los coeficientes de
regresión mediante la matriz de correlación obtenida usando Spearman,
y= β₁0.03929951889239226 + β₂0.4138869957611103 + β₃-0.02379283098767225 +
β₄0.12203927691759384 + β₅-0.4720047863901405
    \end{Verbatim}

    Siendo β₁ asociado a los valores obtenidos por la variable Ácido Cítrico
β₂ asociado a los valores obtenidos por la variable Azúcar Residual β₃
asociado a los valores obtenidos por la variable Dióxido de Azúfre Libre
β₄ asociado a los valores obtenidos por la variable Dióxido de Azúfre
Total β₅ asociado a los valores obtenidos por la variable Alcohol

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{285}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scalerX}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{betas}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{286}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Calculo de RMSE Root Mean Squared Error}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}

\PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{En promedio, las predicciones del modelo tienen un error de aproximadamente:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{rmse}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
En promedio, las predicciones del modelo tienen un error de aproximadamente:
1.2968506359111867
    \end{Verbatim}

    Considerando que las variables del modelo, están estandarizadas, el RMSE
obtenido, indica que en promedio, las predicciones del modelo mediante
el método Kendall, están desviadas de los valores reales en alrededor de
1.2969 desviaciones estándar de la variable objetivo (Densidad).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{287}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{residuales} \PY{o}{=} \PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{scalerY}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{validaciuxf3n-de-los-supuestos-kendall}{%
\paragraph{Validación de los supuestos
(Kendall)}\label{validaciuxf3n-de-los-supuestos-kendall}}

Mediante la evaluación de los supuestos del modelo, presentada a
continuación, se encuentra que hay evidencia significativa para rechazar
la hipótesis nula de homocedasticidad. Por lo tanto, se concluye que hay
heterocedasticidad en los residuos del modelo, adicionalmente, se
rechaza la hipótesis nula en el test Shapiro-Wilk; Los residuos no
siguen una distribución normal.

En resumen, dado que se encuentra evidencia de heterocedasticidad y no
se cumple el supuesto de normalidad de los residuos, no es correcto el
uso del modelo y tendría que revisarse técnicas de modelado alternativas
o ajustes en el modelo para abordar estas deficiencias y mejorar la
precisión de las estimaciones.

Supuesto 1 y supuesto 3 y 4: Los residuales son independientes, Promedio
de los 𝓔ᵢ= 0 y presentan varianza constante

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{288}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{residuales}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agregar línea horizontal en y=0}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_109_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    De acuerdo con el gráfico anterior, se encuentra que los residuos,
parecieran mostrar un comportamiento constante al rededor de cero, sin
embargo pareciera mostrar una leve concentración en valores superiores a
cero.

A continuación se proceder a evaluar mediante tests de Breusch-Pagan y
Durbin-Watson, si el comportamiento de los residuos presentan o no,
heterocedasticidad y autocorrelación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{289}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajusta un modelo auxiliar para explicar la varianza de los residuos}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{X\PYZus{}test}  \PY{c+c1}{\PYZsh{} Puedes utilizar las variables originales o alguna transformación de estas}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X\PYZus{}auxiliar}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agrega una constante si es necesario}
\PY{n}{model\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}auxiliar}\PY{p}{)}
\PY{n}{results\PYZus{}auxiliar} \PY{o}{=} \PY{n}{model\PYZus{}auxiliar}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Calcula el estadístico LM para el test de Breusch\PYZhy{}Pagan}
\PY{n}{BP\PYZus{}statistic} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{rsquared}
\PY{c+c1}{\PYZsh{} Calcula el p\PYZhy{}valor asociado al estadístico LM}
\PY{n}{p\PYZus{}valor\PYZus{}BP} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{f\PYZus{}pvalue}

\PY{c+c1}{\PYZsh{} Imprime los resultados}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{BP\PYZus{}statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor del test de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{p\PYZus{}valor\PYZus{}BP}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Breusch-Pagan: 0.016993546999909848
P-valor del test de Breusch-Pagan: 0.005063412541841041
    \end{Verbatim}

    Teniendo en cuenta los resultados del test de Breusch-Pagan, dado un
valor p significativamente bajo, hay evidencia significativa para
rechazar la hipótesis nula de homocedasticidad. Por lo tanto, se
concluye que hay heterocedasticidad en los residuos del modelo

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{290}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k+kn}{import} \PY{n}{durbin\PYZus{}watson}

\PY{n}{residuos} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{resid}

\PY{c+c1}{\PYZsh{} Calcular el estadístico de Durbin\PYZhy{}Watson}
\PY{n}{statistic} \PY{o}{=} \PY{n}{durbin\PYZus{}watson}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Durbin\PYZhy{}Watson:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{statistic}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Durbin-Watson: 2.022273885522051
    \end{Verbatim}

    Supuesto 2: Los 𝓔ᵢ presentan una distribución Normal

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{291}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frecuencia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_115_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{292}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}import statsmodels.api as sm}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{gofplots} \PY{k}{as} \PY{n+nn}{smg}

\PY{c+c1}{\PYZsh{}  gráfico QQPlot}
\PY{n}{fig} \PY{o}{=} \PY{n}{smg}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Mostrar el gráfico}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_116_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{293}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test de Shapiro\PYZhy{}Wilk para validación de Normalidad en los residuales}

\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}

\PY{n}{shapiro\PYZus{}result} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{shapiro}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado del test}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Shapiro\PYZhy{}Wilk:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Interpretar el resultado del test}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{k}{if} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue} \PY{o}{\PYZgt{}} \PY{n}{alpha}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se puede rechazar la hipótesis nula. Los residuos siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Shapiro-Wilk: 0.9823205650104967
P-valor: 1.5967563982052289e-09
Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.
    \end{Verbatim}

    Calculo del modelo usando Pearson:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{294}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}A continuación se calculan los coeficientes de regresión mediante la expresión anterior de matriz de correlación}
\PY{c+c1}{\PYZsh{}Método pearson}
\PY{c+c1}{\PYZsh{}B0=0 considerando que los datos están estandarizados}

\PY{n}{betas} \PY{o}{=} \PY{n}{RLC}\PY{p}{(}\PY{n}{df\PYZus{}scaled}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pearson}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{betas}
\PY{c+c1}{\PYZsh{}Los valores de Beta mayores, representan mayor dependencia a la variable}

\PY{c+c1}{\PYZsh{} Imprimir el mensaje en pantalla}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{La siguiente es la expresión del modelo calculado, estimando los coeficientes de regresión mediante la matriz de correlación obtenida usando Spearman, }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{y= }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₁}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₂}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₃}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₄}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)} \PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{β₅}\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{betas}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
La siguiente es la expresión del modelo calculado, estimando los coeficientes de
regresión mediante la matriz de correlación obtenida usando Spearman,
y= β₁0.05154898889384292 + β₂0.6046284708827587 + β₃-0.08339784307542122 +
β₄0.12011899954889196 + β₅-0.46400145920757185
    \end{Verbatim}

    Siendo β₁ asociado a los valores obtenidos por la variable Ácido Cítrico
β₂ asociado a los valores obtenidos por la variable Azúcar Residual β₃
asociado a los valores obtenidos por la variable Dióxido de Azúfre Libre
β₄ asociado a los valores obtenidos por la variable Dióxido de Azúfre
Total β₅ asociado a los valores obtenidos por la variable Alcohol

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{295}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scalerX}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}

\PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{betas}\PY{p}{)} \PY{c+c1}{\PYZsh{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{296}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Calculo de RMSE Root Mean Squared Error}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}

\PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{En promedio, las predicciones del modelo tienen un error de aproximadamente:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{rmse}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
En promedio, las predicciones del modelo tienen un error de aproximadamente:
1.3863748432263951
    \end{Verbatim}

    Considerando que las variables del modelo, están estandarizadas, el RMSE
obtenido, indica que en promedio, las predicciones del modelo mediante
el método Pearson, están desviadas de los valores reales en alrededor de
1.38 desviaciones estándar de la variable objetivo (Densidad).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{297}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{residuales} \PY{o}{=} \PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{\PYZhy{}} \PY{n}{scalerY}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{validaciuxf3n-de-los-supuestos-pearson}{%
\paragraph{Validación de los supuestos
(Pearson)}\label{validaciuxf3n-de-los-supuestos-pearson}}

Mediante la evaluación de los supuestos del modelo, presentada a
continuación, se encuentra que hay evidencia significativa para rechazar
la hipótesis nula de homocedasticidad. Por lo tanto, se concluye que hay
heterocedasticidad en los residuos del modelo, adicionalmente, se
rechaza la hipótesis nula en el test Shapiro-Wilk; Los residuos no
siguen una distribución normal.

En resumen, dado que se encuentra evidencia de heterocedasticidad y no
se cumple el supuesto de normalidad de los residuos, no es correcto el
uso del modelo y tendría que revisarse técnicas de modelado alternativas
o ajustes en el modelo para abordar estas deficiencias y mejorar la
precisión de las estimaciones.

Supuesto 1 y supuesto 3 y 4: Los residuales son independientes, Promedio
de los 𝓔ᵢ= 0 y presentan varianza constante

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{298}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{residuales}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agregar línea horizontal en y=0}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_126_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    De acuerdo con el gráfico anterior, se encuentra que los residuos,
parecieran mostrar un comportamiento constante al rededor de cero.

A continuación se proceder a evaluar mediante tests de Breusch-Pagan y
Durbin-Watson, si el comportamiento de los residuos presentan o no,
heterocedasticidad y autocorrelación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{299}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajusta un modelo auxiliar para explicar la varianza de los residuos}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{X\PYZus{}test}  \PY{c+c1}{\PYZsh{} Puedes utilizar las variables originales o alguna transformación de estas}
\PY{n}{X\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X\PYZus{}auxiliar}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Agrega una constante si es necesario}
\PY{n}{model\PYZus{}auxiliar} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{square}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}\PY{p}{,} \PY{n}{X\PYZus{}auxiliar}\PY{p}{)}
\PY{n}{results\PYZus{}auxiliar} \PY{o}{=} \PY{n}{model\PYZus{}auxiliar}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Calcula el estadístico LM para el test de Breusch\PYZhy{}Pagan}
\PY{n}{BP\PYZus{}statistic} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{rsquared}
\PY{c+c1}{\PYZsh{} Calcula el p\PYZhy{}valor asociado al estadístico LM}
\PY{n}{p\PYZus{}valor\PYZus{}BP} \PY{o}{=} \PY{n}{results\PYZus{}auxiliar}\PY{o}{.}\PY{n}{f\PYZus{}pvalue}

\PY{c+c1}{\PYZsh{} Imprime los resultados}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{BP\PYZus{}statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor del test de Breusch\PYZhy{}Pagan:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{p\PYZus{}valor\PYZus{}BP}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Breusch-Pagan: 0.04044358779539037
P-valor del test de Breusch-Pagan: 1.315862938375713e-07
    \end{Verbatim}

    Teniendo en cuenta los resultados del test de Breusch-Pagan, dado un
valor p significativamente bajo, hay evidencia significativa para
rechazar la hipótesis nula de homocedasticidad. Por lo tanto, se
concluye que hay heterocedasticidad en los residuos del modelo

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{300}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{stattools} \PY{k+kn}{import} \PY{n}{durbin\PYZus{}watson}

\PY{n}{residuos} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{resid}

\PY{c+c1}{\PYZsh{} Calcular el estadístico de Durbin\PYZhy{}Watson}
\PY{n}{statistic} \PY{o}{=} \PY{n}{durbin\PYZus{}watson}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Durbin\PYZhy{}Watson:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{statistic}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Durbin-Watson: 1.996266746756021
    \end{Verbatim}

    Dado que el valor obtenido para el estadístico de Durbin-Watson es
aproximadamente 2.0375, está cerca de 2, pareciera que no hay
autocorrelación de primer orden en los residuos. En consecuencia, los
residuos parecen ser independientes entre sí.

Supuesto 2: Los 𝓔ᵢ presentan una distribución Normal

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{301}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frecuencia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_132_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{302}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}import statsmodels.api as sm}
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{gofplots} \PY{k}{as} \PY{n+nn}{smg}

\PY{c+c1}{\PYZsh{}  gráfico QQPlot}
\PY{n}{fig} \PY{o}{=} \PY{n}{smg}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{residuales}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Mostrar el gráfico}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_133_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{303}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test de Shapiro\PYZhy{}Wilk para validación de Normalidad en los residuales}

\PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k+kn}{import} \PY{n}{stats}

\PY{n}{shapiro\PYZus{}result} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{shapiro}\PY{p}{(}\PY{n}{residuales}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir el resultado del test}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico de Shapiro\PYZhy{}Wilk:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{statistic}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P\PYZhy{}valor:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Interpretar el resultado del test}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{k}{if} \PY{n}{shapiro\PYZus{}result}\PY{o}{.}\PY{n}{pvalue} \PY{o}{\PYZgt{}} \PY{n}{alpha}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se puede rechazar la hipótesis nula. Los residuos siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico de Shapiro-Wilk: 0.9757908607053407
P-valor: 1.0485793225894429e-11
Se rechaza la hipótesis nula. Los residuos no siguen una distribución normal.
    \end{Verbatim}

    Construimos el modelo con las variables transformadas:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{304}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} Obtener solo las columnas de df\PYZus{}scaled que deseas graficar}
\PY{n}{variables} \PY{o}{=} \PY{n}{df3}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Iterar sobre las columnas y graficar cada una contra la variable \PYZdq{}Densidad\PYZdq{}}
\PY{k}{for} \PY{n}{column} \PY{o+ow}{in} \PY{n}{variables}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{df3}\PY{p}{[}\PY{n}{column}\PY{p}{]}\PY{p}{,} \PY{n}{df3}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{column}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Densidad}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Diagrama de dispersión de }\PY{l+s+s1}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{column}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{ vs. }\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{Densidad}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_136_4.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{305}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Modelo Alcohol vs Densidad sin considerar transformación de variable}

\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{X} \PY{o}{=} \PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alcohol}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}   \PY{c+c1}{\PYZsh{} Variable explicativa}
\PY{n}{y} \PY{o}{=} \PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Densidad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}      \PY{c+c1}{\PYZsh{} Variable objetivo}

\PY{c+c1}{\PYZsh{} Agregar intercepto al conjunto de datos}
\PY{n}{X} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir los resultados del modelo}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:               Densidad   R-squared:                       0.609
Model:                            OLS   Adj. R-squared:                  0.609
Method:                 Least Squares   F-statistic:                     7613.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):               0.00
Time:                        19:33:30   Log-Likelihood:                 23816.
No. Observations:                4898   AIC:                        -4.763e+04
Df Residuals:                    4896   BIC:                        -4.761e+04
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.0140      0.000   4407.871      0.000       1.014       1.014
Alcohol       -0.0019   2.17e-05    -87.255      0.000      -0.002      -0.002
==============================================================================
Omnibus:                     4515.647   Durbin-Watson:                   1.745
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1413293.678
Skew:                           3.649   Prob(JB):                         0.00
Kurtosis:                      85.896   Cond. No.                         91.9
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{306}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alcohol\PYZus{}inv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alcohol}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{307}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Modelo Alcohol vs Densidad considerando trasnformación de Variable 1/Alcohol}

\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{X} \PY{o}{=} \PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alcohol\PYZus{}inv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}   \PY{c+c1}{\PYZsh{} Variable explicativa}
\PY{n}{y} \PY{o}{=} \PY{n}{df3}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Densidad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}      \PY{c+c1}{\PYZsh{} Variable objetivo}

\PY{c+c1}{\PYZsh{} Agregar intercepto al conjunto de datos}
\PY{n}{X} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Ajustar el modelo de regresión lineal}
\PY{n}{model} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Imprimir los resultados del modelo}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:               Densidad   R-squared:                       0.622
Model:                            OLS   Adj. R-squared:                  0.622
Method:                 Least Squares   F-statistic:                     8043.
Date:                Sun, 28 Apr 2024   Prob (F-statistic):               0.00
Time:                        19:33:30   Log-Likelihood:                 23899.
No. Observations:                4898   AIC:                        -4.779e+04
Df Residuals:                    4896   BIC:                        -4.778e+04
Df Model:                           1
Covariance Type:            nonrobust
===============================================================================
                  coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------
const           0.9731      0.000   4153.950      0.000       0.973       0.974
Alcohol\_inv     0.2166      0.002     89.683      0.000       0.212       0.221
==============================================================================
Omnibus:                     4664.261   Durbin-Watson:                   1.746
Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1652083.297
Skew:                           3.832   Prob(JB):                         0.00
Kurtosis:                      92.646   Cond. No.                         92.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    A partir de la transformación realizada la variable ``Alcohol'', se
realiza un análisis comparativo de modelos OLS con, y sin
transformación, de donde se tiene que:

Modelo 1 (sin transformación de la variable `alcohol'):

\(Adj - R^2\): 0.609

Coeficiente para la variable Alcohol: -0.0019

Modelo 2 (transformando la variable `Alcohol'):

\(Adj - R^2\): 0.622 Coeficiente para la variable Alcohol\_inv: 0.2166

El modelo 2 tiene un R-cuadrado ajustado ligeramente mayor (0.622) en
comparación con el modelo 1 (0.609), lo que sugiere que explica una
mayor proporción de la variabilidad en la variable de respuesta
(Densidad). Además, el coeficiente para la variable Alcohol\_inv en el
modelo 2 es significativamente diferente de cero y tiene un efecto
positivo en la variable de respuesta.

Por lo tanto, en base a estos resultados, se podría concluir que el
MODELO 2 es mejor en términos de ajuste y explicación de la variabilidad
en la variable de respuesta.

    \hypertarget{ejercicio-de-regresion-lineal-simple}{%
\subsubsection{Ejercicio de regresion lineal
simple}\label{ejercicio-de-regresion-lineal-simple}}

    El ejercicio que nos concierne en este nuevo apartado se refiere a un
conjunto de datos que registra la cantidad de anuncios publicitarios en
redes sociales que realiza una empresa y su correspondiente retorno de
inversión en ventas. Se desea determinar si existe una relación lineal
significativa entre la cantidad de anuncios publicitarios y el retorno
de inversión.

El dataset nos da información acerca de los gastos en publicidad (en
miles de dólares) y las ventas (en miles de unidades) de un producto en
un mercado específico:

\begin{itemize}
\tightlist
\item
  \textbf{TV}: Gasto (miles de dólares) en publicidad en televisión.\\
\item
  \textbf{Radio}: Gasto (miles de dólares) en publicidad en radio.\\
\item
  \textbf{Newspaper}: Gasto (miles de dólares) en publicidad en
  periódicos.\\
\item
  \textbf{Sales}: Número de unidades vendidas (en miles)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{308}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Cargamos el dataset y entendemos su estructura}

\PY{n}{publicidad} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D:}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{OneDrive \PYZhy{} Tecnoquimicas}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{99. PERSONAL}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Formación}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Maestria}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Semestre 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Analisis Cuantitivo}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Trabajo 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{publicidad.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{publicidad}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
 \#   Column      Non-Null Count  Dtype
---  ------      --------------  -----
 0   Unnamed: 0  200 non-null    int64
 1   TV          200 non-null    float64
 2   Radio       200 non-null    float64
 3   Newspaper   200 non-null    float64
 4   Sales       200 non-null    float64
dtypes: float64(4), int64(1)
memory usage: 7.9 KB
    \end{Verbatim}

    Nos damos cuenta que existe una columna adicional de las declaradas en
el enunciado del ejercicio, con nombre `Unnamed: 0'. Procedemos a llamar
el metodo `head' con el fin de visualizar la estructura del df y qué
datos registra esta columna.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{309}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{publicidad}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{309}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Unnamed: 0     TV  Radio  Newspaper  Sales
0           1  230.1   37.8       69.2   22.1
1           2   44.5   39.3       45.1   10.4
2           3   17.2   45.9       69.3    9.3
3           4  151.5   41.3       58.5   18.5
4           5  180.8   10.8       58.4   12.9
5           6    8.7   48.9       75.0    7.2
6           7   57.5   32.8       23.5   11.8
7           8  120.2   19.6       11.6   13.2
8           9    8.6    2.1        1.0    4.8
9          10  199.8    2.6       21.2   10.6
\end{Verbatim}
\end{tcolorbox}
        
    Al llamar a los primeros 10 registros, nos damos cuenta que la columna
`Unnamed : 0' es un contador de registros que inicia en 1. Sin embargo,
no es relevante para el ejercicio aquí descrito, por lo que procedemos a
eliminarla.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{310}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{publicidad}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unnamed: 0}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}\PY{n}{inplace}\PY{o}{=} \PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} Eliminamos la columna}
\PY{n}{publicidad}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}Revisamos si quedó grabado}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{310}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
      TV  Radio  Newspaper  Sales
0  230.1   37.8       69.2   22.1
1   44.5   39.3       45.1   10.4
2   17.2   45.9       69.3    9.3
3  151.5   41.3       58.5   18.5
4  180.8   10.8       58.4   12.9
\end{Verbatim}
\end{tcolorbox}
        
    Procedemos a graficar la distribución de las variables y la relación
entre pares:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{311}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Graficas scatter matrix y dimensionamos}
\PY{n}{sn}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{publicidad}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_149_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al graficar la scatter matrix podemos evidenciar que:

\begin{itemize}
\tightlist
\item
  La relación entre el gasto en publicidad en TV vs las Ventas parece de
  dependencia positiva; sin embargo, conforme el gasto en publicidad
  aumenta, parece haber mayor variabilidad de los datos. Nos puede
  inidicar la presencia de heterocedasticidad.
\item
  La relacion entre el gatos en publicidad en Radio vs las Ventas
  también parece presentar una dependencia positiva como en el apartado
  anterior, aunque más leve y con una mayor dispersión en los puntos.
\item
  Entre el gasto en prensa y las ventas no parece haber una dependencia
  notoria, existe una mayor dispersión de los datos. No obstante, se
  debe hacer toda la estadística para cuantificar las correlaciones
  entre las variables.
\end{itemize}

    De acuerdo a lo anterior, procedemos a calcular el coeficiente de
correlación entre todas las variables y las graficamos mediante un mapa
de calor

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{312}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{plotly}\PY{n+nn}{.}\PY{n+nn}{express} \PY{k}{as} \PY{n+nn}{px}

\PY{n}{corr\PYZus{}matrix} \PY{o}{=} \PY{n}{publicidad}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}Construimos la matriz de correlaciones}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                 TV     Radio  Newspaper     Sales
TV         1.000000  0.054809   0.056648  0.782224
Radio      0.054809  1.000000   0.354104  0.576223
Newspaper  0.056648  0.354104   1.000000  0.228299
Sales      0.782224  0.576223   0.228299  1.000000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{313}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Graficamos el mapa de calor interactivo}
\PY{n}{fig} \PY{o}{=} \PY{n}{px}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{314}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sn}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr\PYZus{}matrix}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mako}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_154_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Teniendo en cuenta los resultados arrojados, podemos corroborar las
afirmaciones anteriores: la variable \textbf{``Sales''} presenta una
relacion directa con las tres variables independientes (\textbf{``TV''},
\textbf{``Radio''}, \textbf{``Newspaper''}), siendo TV la más alta con
una fuerte correlacion de 0.78, seguido de Radio con una correlacion de
0.57 y prensa, con una correlación débil de 0.22.

Con el fin de adelantar un modelo de regresión simple, y siendo
coherentes con los resultados del punto anterior, planteamos el modelo
donde \textbf{``Sales''} actúa como variable dependiente y \textbf{TV}
como variable explicativa, siendo que es la que mayor correlación
presenta. Así, el modelo:

\[\hat{y} = \hat{\beta_0} + \hat{\beta_1}TV\]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{315}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Corremos el modelo con la interacción eliminando la variable categórica Ind como predictor solo}
\PY{n}{TV} \PY{o}{=} \PY{n}{publicidad}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{VTA} \PY{o}{=} \PY{n}{publicidad}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{model\PYZus{}pub} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{VTA}\PY{p}{,}\PY{n}{TV}\PY{p}{)}

\PY{n}{results\PYZus{}pub} \PY{o}{=} \PY{n}{model\PYZus{}pub}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{results\PYZus{}pub}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results
==============================================================================
Dep. Variable:                  Sales   R-squared:                       0.612
Model:                            OLS   Adj. R-squared:                  0.610
Method:                 Least Squares   F-statistic:                     312.1
Date:                Sun, 28 Apr 2024   Prob (F-statistic):           1.47e-42
Time:                        19:33:33   Log-Likelihood:                -519.05
No. Observations:                 200   AIC:                             1042.
Df Residuals:                     198   BIC:                             1049.
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
TV             0.0475      0.003     17.668      0.000       0.042       0.053
const          7.0326      0.458     15.360      0.000       6.130       7.935
==============================================================================
Omnibus:                        0.531   Durbin-Watson:                   1.935
Prob(Omnibus):                  0.767   Jarque-Bera (JB):                0.669
Skew:                          -0.089   Prob(JB):                        0.716
Kurtosis:                       2.779   Cond. No.                         338.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
    \end{Verbatim}

    Al correr el modelo se calcula el valor de los coeficientes, que permite
estimar la función de la recta como:

\[\hat{y} = 7.0326 + 0.0475*TV\]

Con un \(R^2\) del 0.612. Es decir, el modelo ajusta a los datos en un
61.2\%, o lo que es lo mismo, el modelo puede explicar el 61.2\% de la
variablidad de la variable respuesta.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{316}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Graficamos la dispersión de las variables con la linea de regresión}
\PY{n}{sn}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{publicidad}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{line\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{palette}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mako}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} \PY{n}{aspect}\PY{o}{=}\PY{l+m+mi}{7}\PY{o}{/}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{316}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<seaborn.axisgrid.FacetGrid at 0x14d9756ebe0>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_158_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Hemos hecho explicito el modelo de regresión que mejor se ajusta a los
datos. Ahora, si quisieramos predecir la venta que generarían 5 nuevos
anuncios en la TV, ¿qué nos arrojaría el modelo? Primero, generemos 5
nuevos datos al azar dentro del rango actual de los datos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{317}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Calculamos las estadísticas descriptivas de la variable independiente}
\PY{n}{publicidad}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{317}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
count    200.000000
mean     147.042500
std       85.854236
min        0.700000
25\%       74.375000
50\%      149.750000
75\%      218.825000
max      296.400000
Name: TV, dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{318}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{random}

\PY{c+c1}{\PYZsh{}Genera una lista con numeros al azar dentro del rango del valor minimo y el valor máximo}

\PY{n}{random\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{}lista vacía}

\PY{c+c1}{\PYZsh{}Itero la generación del número al azar y lo guardo en lista}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n+nb}{round}\PY{p}{(}\PY{n}{random}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{296.4}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{random\PYZus{}list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{x}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{random\PYZus{}list}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[175.8, 87.93, 241.36, 175.45, 249.72]
    \end{Verbatim}

    Convertimos el nuevo set de datos en un df para hacer más fácil su
lectura y aplicación.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{319}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Se crea un DF a partir de la lista para permitir el cálculo}
\PY{n}{tv} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{random\PYZus{}list}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{tv}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{319}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
        0  const
0  175.80      1
1   87.93      1
2  241.36      1
3  175.45      1
4  249.72      1
\end{Verbatim}
\end{tcolorbox}
        
    Generamos los \(\hat{y}\) a partir de los nuevos datos y calculamos el
intervalo de confianza.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{320}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Con la data generada arriba se predicen las siguientes ventas}
\PY{n}{sales\PYZus{}hat} \PY{o}{=} \PY{n}{results\PYZus{}pub}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{tv}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{sales\PYZus{}hat}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0    15.389535
1    11.212490
2    18.506037
3    15.372897
4    18.903443
dtype: float64
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{321}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Calculamos el intervalo de confianza del 95\PYZpc{}}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k}{as} \PY{n+nn}{st}
\PY{n}{st}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{interval}\PY{p}{(}\PY{n}{df}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sales\PYZus{}hat}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{sales\PYZus{}hat}\PY{p}{)}\PY{p}{,} \PY{n}{scale}\PY{o}{=}\PY{n}{st}\PY{o}{.}\PY{n}{sem}\PY{p}{(}\PY{n}{sales\PYZus{}hat}\PY{p}{)}\PY{p}{,} \PY{n}{confidence}\PY{o}{=}\PY{l+m+mf}{0.95}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{321}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(12.033705452340925, 19.72005569760285)
\end{Verbatim}
\end{tcolorbox}
        
    De esta manera,si aplicamos el modelo de regresión elegido al set de
datos generados en el apartado anterior, se espera que en promedio se
reporten ventas entre el 7.7 y 18.65 unidades con una confianza del
95\%.

    \hypertarget{ejercicio-de-regresiuxf3n-lineal-muxfaltiple}{%
\subsubsection{Ejercicio de regresión lineal
múltiple}\label{ejercicio-de-regresiuxf3n-lineal-muxfaltiple}}

Se desea predecir la resistencia a la compresión del concreto (Concrete
compressive strength) en función de diferentes variables predictoras
como el cemento (Cement), la escoria (Slag), la ceniza volante (Fly
ash), el agua (Water), el superplastificante (Superplasticizer), el
agregado grueso (Coarse aggregate) y el agregado fino (Fine aggregate).
Para ello se dispone de un conjunto de datos con 1030 observaciones. Se
desea construir un modelo de regresión lineal múltiple para predecir la
resistencia a la compresión del concreto en función de las variables
predictoras.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{322}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Importar el data set}
\PY{n}{concrete} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}excel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D:}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{OneDrive \PYZhy{} Tecnoquimicas}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{99. PERSONAL}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Formación}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Maestria}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Semestre 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Analisis Cuantitivo}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Trabajo 1}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Concrete\PYZus{}Data.xls}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{sheet\PYZus{}name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sheet1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Entender la estructura y características del dataset.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{323}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{concrete}\PY{o}{.}\PY{n}{shape}
\PY{n}{concrete}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1030 entries, 0 to 1029
Data columns (total 9 columns):
 \#   Column                                                 Non-Null Count
Dtype
---  ------                                                 --------------
-----
 0   Cement (component 1)(kg in a m\^{}3 mixture)              1030 non-null
float64
 1   Blast Furnace Slag (component 2)(kg in a m\^{}3 mixture)  1030 non-null
float64
 2   Fly Ash (component 3)(kg in a m\^{}3 mixture)             1030 non-null
float64
 3   Water  (component 4)(kg in a m\^{}3 mixture)              1030 non-null
float64
 4   Superplasticizer (component 5)(kg in a m\^{}3 mixture)    1030 non-null
float64
 5   Coarse Aggregate  (component 6)(kg in a m\^{}3 mixture)   1030 non-null
float64
 6   Fine Aggregate (component 7)(kg in a m\^{}3 mixture)      1030 non-null
float64
 7   Age (day)                                              1030 non-null
int64
 8   Concrete compressive strength(MPa, megapascals)        1030 non-null
float64
dtypes: float64(8), int64(1)
memory usage: 72.5 KB
    \end{Verbatim}

    Con el fin de simplificar el ejercicio, procedemos a renombrar las
columnas.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{324}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{headers} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Cement}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Blast Furnace Slag}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fly Ash}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Water}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Superplasticizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coarse Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fine Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{concrete}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{n}{headers}
\PY{n}{concrete}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{324}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \textbackslash{}
0   540.0                 0.0      0.0  162.0               2.5
1   540.0                 0.0      0.0  162.0               2.5
2   332.5               142.5      0.0  228.0               0.0
3   332.5               142.5      0.0  228.0               0.0
4   198.6               132.4      0.0  192.0               0.0

   Coarse Aggregate  Fine Aggregate  Age  Concrete Compressive Strength
0            1040.0           676.0   28                      79.986111
1            1055.0           676.0   28                      61.887366
2             932.0           594.0  270                      40.269535
3             932.0           594.0  365                      41.052780
4             978.4           825.5  360                      44.296075
\end{Verbatim}
\end{tcolorbox}
        
    Aprovechamos para graficar la distribución de las variables y su
relación entre pares a través de una scatter matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{325}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Graficas scatter matrix y dimensionamos}
\PY{n}{sn}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{concrete}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{325}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<seaborn.axisgrid.PairGrid at 0x14d99192520>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_175_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    La anterior grafica nos permite evidenciar:

\begin{itemize}
\item
  La variable dependiente \textbf{`Cement Compressive Strength'} parece
  tener una relación de dependencia positiva con las variables
  \textbf{`Cement'} y \textbf{`Superplasticizer'}, y una dependencia
  negativa con la variable \textbf{`Water'}. La relacion de dependencia
  de \textbf{`Cement Compressive Strength'} con las demás variables no
  es tan aparente, por lo que debemos calcular la metrica de
  correlaciones para conocer la fuerza de esa relación.
\item
  Algunos de los atributos muestran cierta relación de dependencia:
  \textbf{`Water'} \& \textbf{`Superplasticizer'} parecen tener una
  relación de dependencia negativa, así como la primera con
  \textbf{`Coarse Agregate'}. Para la relación de las demás variables
  independientes no podemos acercarnos a una conclusión sin calcular la
  matriz de correlación que se propone en el punto anterior.
\end{itemize}

Por lo mismo, procedemos a generar una matriz de correlación y el
respectivo gráfico de mapa de calor:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{326}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{corr\PYZus{}concrete} \PY{o}{=} \PY{n}{concrete}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}Construimos la matriz de correlaciones}
\PY{n}{corr\PYZus{}concrete}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{326}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
                                 Cement  Blast Furnace Slag   Fly Ash  \textbackslash{}
Cement                         1.000000           -0.275193 -0.397475
Blast Furnace Slag            -0.275193            1.000000 -0.323569
Fly Ash                       -0.397475           -0.323569  1.000000
Water                         -0.081544            0.107286 -0.257044
Superplasticizer               0.092771            0.043376  0.377340
Coarse Aggregate              -0.109356           -0.283998 -0.009977
Fine Aggregate                -0.222720           -0.281593  0.079076
Age                            0.081947           -0.044246 -0.154370
Concrete Compressive Strength  0.497833            0.134824 -0.105753

                                  Water  Superplasticizer  Coarse Aggregate  \textbackslash{}
Cement                        -0.081544          0.092771         -0.109356
Blast Furnace Slag             0.107286          0.043376         -0.283998
Fly Ash                       -0.257044          0.377340         -0.009977
Water                          1.000000         -0.657464         -0.182312
Superplasticizer              -0.657464          1.000000         -0.266303
Coarse Aggregate              -0.182312         -0.266303          1.000000
Fine Aggregate                -0.450635          0.222501         -0.178506
Age                            0.277604         -0.192717         -0.003016
Concrete Compressive Strength -0.289613          0.366102         -0.164928

                               Fine Aggregate       Age  \textbackslash{}
Cement                              -0.222720  0.081947
Blast Furnace Slag                  -0.281593 -0.044246
Fly Ash                              0.079076 -0.154370
Water                               -0.450635  0.277604
Superplasticizer                     0.222501 -0.192717
Coarse Aggregate                    -0.178506 -0.003016
Fine Aggregate                       1.000000 -0.156094
Age                                 -0.156094  1.000000
Concrete Compressive Strength       -0.167249  0.328877

                               Concrete Compressive Strength
Cement                                              0.497833
Blast Furnace Slag                                  0.134824
Fly Ash                                            -0.105753
Water                                              -0.289613
Superplasticizer                                    0.366102
Coarse Aggregate                                   -0.164928
Fine Aggregate                                     -0.167249
Age                                                 0.328877
Concrete Compressive Strength                       1.000000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{327}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Graficamos el mapa de calor interactivo}
\PY{n}{fig} \PY{o}{=} \PY{n}{px}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{corr\PYZus{}concrete}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{328}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{sn}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{corr\PYZus{}concrete}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mako}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_179_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    El apartado anterior nos permite corroborar lo inferido en la grafica de
\emph{scatter matrix}:

\begin{itemize}
\tightlist
\item
  La variable dependiente \textbf{`Concrete Compressive Strength'} en
  efecto presenta una relación de dependencia positiva moderada de 0.49
  con el componente 1 \textbf{`Concrete'} y dependencia débil de 0.36
  con \textbf{`Superplasticizer'}, así como con \textbf{`Age'} con un
  0.32 de correlación. Además, presenta una relación de dependencia
  negativa con la variable \textbf{`Water'} de 0.28. Presenta una
  relación de dependencia debil con los demás atributos.
\end{itemize}

    Una vez conocemos la relación entre las variables, proponemos un modelo
de regresión múltiple que iremos refinando conforme identificamos las
variables significativas.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{329}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{concrete}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{Y} \PY{o}{=} \PY{n}{concrete}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{mod} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y}\PY{p}{,}\PY{n}{X}\PY{p}{)}
\PY{n}{mod\PYZus{}results} \PY{o}{=} \PY{n}{mod}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                                  OLS Regression Results
================================================================================
=========
Dep. Variable:     Concrete Compressive Strength   R-squared:
0.615
Model:                                       OLS   Adj. R-squared:
0.612
Method:                            Least Squares   F-statistic:
204.3
Date:                           Sun, 28 Apr 2024   Prob (F-statistic):
6.76e-206
Time:                                   19:33:46   Log-Likelihood:
-3869.0
No. Observations:                           1030   AIC:
7756.
Df Residuals:                               1021   BIC:
7800.
Df Model:                                      8
Covariance Type:                       nonrobust
================================================================================
======
                         coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
------
Cement                 0.1198      0.008     14.110      0.000       0.103
0.136
Blast Furnace Slag     0.1038      0.010     10.245      0.000       0.084
0.124
Fly Ash                0.0879      0.013      6.988      0.000       0.063
0.113
Water                 -0.1503      0.040     -3.741      0.000      -0.229
-0.071
Superplasticizer       0.2907      0.093      3.110      0.002       0.107
0.474
Coarse Aggregate       0.0180      0.009      1.919      0.055      -0.000
0.036
Fine Aggregate         0.0202      0.011      1.883      0.060      -0.001
0.041
Age                    0.1142      0.005     21.046      0.000       0.104
0.125
const                -23.1638     26.588     -0.871      0.384     -75.338
29.010
==============================================================================
Omnibus:                        5.379   Durbin-Watson:                   1.281
Prob(Omnibus):                  0.068   Jarque-Bera (JB):                5.305
Skew:                          -0.174   Prob(JB):                       0.0705
Kurtosis:                       3.045   Cond. No.                     1.06e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
[2] The condition number is large, 1.06e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
    \end{Verbatim}

    Al correr el modelo, vemos que las variables \textbf{`Coarse Aggregate'}
\& \textbf{`Fine Aggregate'} arrojan un p-valor mayor a 0.05, por lo que
podríamos descartarlas al no ser significativas para el modelo. Corremos
nuevamente el modelo, deshaciendonos de ambas variables. El modelo se
ajusta un 61.2\% a los datos, o lo que es lo mismo, explica el 61\% de
la variabilidad de los datos.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{330}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{concrete}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coarse Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fine Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{Y} \PY{o}{=} \PY{n}{concrete}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{mod} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y}\PY{p}{,}\PY{n}{X}\PY{p}{)}
\PY{n}{mod\PYZus{}results} \PY{o}{=} \PY{n}{mod}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                                  OLS Regression Results
================================================================================
=========
Dep. Variable:     Concrete Compressive Strength   R-squared:
0.614
Model:                                       OLS   Adj. R-squared:
0.612
Method:                            Least Squares   F-statistic:
271.2
Date:                           Sun, 28 Apr 2024   Prob (F-statistic):
1.78e-207
Time:                                   19:33:47   Log-Likelihood:
-3871.0
No. Observations:                           1030   AIC:
7756.
Df Residuals:                               1023   BIC:
7791.
Df Model:                                      6
Covariance Type:                       nonrobust
================================================================================
======
                         coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
------
Cement                 0.1054      0.004     24.821      0.000       0.097
0.114
Blast Furnace Slag     0.0865      0.005     17.386      0.000       0.077
0.096
Fly Ash                0.0687      0.008      8.881      0.000       0.054
0.084
Water                 -0.2183      0.021    -10.332      0.000      -0.260
-0.177
Superplasticizer       0.2390      0.085      2.826      0.005       0.073
0.405
Age                    0.1135      0.005     20.987      0.000       0.103
0.124
const                 29.0302      4.212      6.891      0.000      20.764
37.296
==============================================================================
Omnibus:                        5.233   Durbin-Watson:                   1.286
Prob(Omnibus):                  0.073   Jarque-Bera (JB):                5.193
Skew:                          -0.174   Prob(JB):                       0.0745
Kurtosis:                       3.019   Cond. No.                     4.66e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
[2] The condition number is large, 4.66e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
    \end{Verbatim}

    Al eliminar las variables \textbf{`Coarse Aggregate'} \& \textbf{`Fine
Aggregate'} vemos que los demás atributos se mantienen significativos;
sin embargo, el \(Adj. R^2\) no mejora, indicandonos que estas variables
no añadían distorsión adicional al modelo ni explicaban parte de la
variabilidad de los datos.

No obstante, al usar los criterios de información podemos comparar ambos
modelos y discernir sobre su calidad: el criterio de Akaike no muestra
una mejora, sin embargo, el criterio de Bayes presenta una mejoría en el
segundo modelo, pasando de 7800 a 7791. Podemos así concluir que el
segundo modelo, donde todas las variables son significativas y presenta
un ajuste del 61.2\%, es relativamente de mejor calidad que el primer
modelo, donde se usan todas los atributos disponibles.

    \hypertarget{validacion-de-supuestos-del-modelo}{%
\paragraph{Validacion de supuestos del
modelo}\label{validacion-de-supuestos-del-modelo}}

Una vez planteado el modelo, procedemos a validar cada uno de los
supuestos del modelo,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Iniciamos con la validación de la independencia de los errores.
  Adelantamos el análisis gráfico con correlogramas, seguido de las
  pruebas formales.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{331}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{graphics}\PY{n+nn}{.}\PY{n+nn}{tsaplots} \PY{k+kn}{import} \PY{n}{plot\PYZus{}acf}\PY{p}{,} \PY{n}{plot\PYZus{}pacf}

\PY{c+c1}{\PYZsh{} Test 1}
\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}

\PY{n}{plot\PYZus{}acf}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ACF de la la fuerza de compresión del concreto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}
\PY{n}{plot\PYZus{}pacf}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{lags}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PACF de la fuerza de compresión del concreto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}

\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{]}\PY{p}{)}
\PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{1.2}\PY{p}{]}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{subplots\PYZus{}adjust}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_187_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al construir el grafico de autocorrelación simple y autocorrelación
parcial de la variable dependiente, aparentemente vemos que, con un lag
de 5 datos, cada uno de ellos es significativo y nos llevaría a pensar
que no existe independencia de los errores. Además, al correr el modelo
nos arroja un valor de Durbin - Watson de 1.286, alejado del 2 ideal, lo
que nos reforzaría la idea de autocorrelación.

Corremos la prueba de Breusch - Godfrey definiendo el método y
presentando una tabla de validación de hipótesis, que se definen así:

\[ H_0: \rho(\epsilon_{i}, \epsilon_{i+1}) = 0 \\
H_1: \rho(\epsilon_{i}, \epsilon_{i+1}) \neq 0 \]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{332}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{stats}\PY{n+nn}{.}\PY{n+nn}{diagnostic} \PY{k+kn}{import} \PY{n}{acorr\PYZus{}breusch\PYZus{}godfrey}\PY{p}{,} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{,} \PY{n}{het\PYZus{}white}

\PY{c+c1}{\PYZsh{} Test 3}
\PY{k}{def} \PY{n+nf}{test\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{model\PYZus{}results}\PY{p}{,} \PY{n}{maxlags}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{list} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{maxlags}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{values} \PY{o}{=} \PY{n}{acorr\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{model\PYZus{}results}\PY{p}{,} \PY{n}{nlags}\PY{o}{=}\PY{n}{i}\PY{p}{)}
        \PY{n+nb}{list}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
    
    \PY{n}{table} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n+nb}{list}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lags}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
    \PY{n}{table}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lags}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} P\PYZhy{}value \PYZlt{} pvalue }
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.1}
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.05}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.05}
    \PY{n}{table}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pv\PYZlt{}0.01}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.01}

    \PY{c+c1}{\PYZsh{} Rounding}
    \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
    \PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{table}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pvalue}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

    \PY{k}{return} \PY{n}{table}


\PY{c+c1}{\PYZsh{} La hipotesis nula es de no autocorrelacion}
\PY{c+c1}{\PYZsh{} Se rechaza la no autocorrelacion hasta 5 rezagos. }
\PY{n}{test\PYZus{}breusch\PYZus{}godfrey}\PY{p}{(}\PY{n}{mod\PYZus{}results}\PY{p}{,} \PY{n}{maxlags}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{332}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           LM  pvalue  pv<0.1  pv<0.05  pv<0.01
lags
1     131.737     0.0    True     True     True
2     138.174     0.0    True     True     True
3     143.549     0.0    True     True     True
4     170.467     0.0    True     True     True
5     215.916     0.0    True     True     True
\end{Verbatim}
\end{tcolorbox}
        
    Al correr el test de Breusch - Godfrey hasta en 5 rezagos podemos
rechazar la hipótesis nula y concluir que los errores no son
independiente, existe \textbf{autocorrelación}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Continuamos con validación de la varianza constante de los errores. Al
  igual que en el apartado pasado, iniciamos con una análisis gráfico y
  procedemos a correr las pruebas formales.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{333}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test gráfico}
\PY{n}{y\PYZus{}hat} \PY{o}{=} \PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\PY{n}{residuos} \PY{o}{=} \PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{resid}

\PY{n}{sn}\PY{o}{.}\PY{n}{scatterplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{n}{y\PYZus{}hat}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{residuos}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{axhline}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Analisis grafico de los residuos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fuerza de compresión del concreto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuo}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{333}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0, 0.5, 'Residuo')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_191_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al graficar el comportamiento de las predicciones con los errores no es
muy claro si existe homocedasticidad; sin embargo, conforme aumenta la
variable y los errores se ven más dipersos. Por lo mismo, adelantamos
las pruebas formales de Breusch - Pagan y White con el fin de corroborar
si existe o no heterocedasticidad. Por lo mismo, planteamos la prueba de
hipótesis:

\[ H_0: \mathbb{V}(\epsilon_{i}) = Constante \\
H_1: \mathbb{V}(\epsilon_{i})  \neq Constante \]

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{334}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Test 2}
\PY{n}{BP\PYZus{}test} \PY{o}{=} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{(}\PY{n}{residuos}\PY{p}{,} \PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{BP\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{BP\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El estadistico Breusch \PYZhy{} Pagan es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ y el p\PYZhy{}value es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}test}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El estadistico Breusch - Pagan es 139.182 y el p-value es 0.0
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{335}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test 3}
\PY{n}{white\PYZus{}test} \PY{o}{=} \PY{n}{het\PYZus{}white}\PY{p}{(}\PY{n}{residuos}\PY{p}{,} \PY{n}{mod\PYZus{}results}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estadístico White}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Statistic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Test p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{white\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{white\PYZus{}test}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{labels}\PY{p}{,} \PY{n}{white\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'Estadístico White': 258.69, 'p-value': 0.0, 'F-Statistic': 12.447, 'F-Test
p-value': 0.0\}
    \end{Verbatim}

    Ambas pruebas nos dan un p-value menor a 0.05, lo que nos lleva a
rechazar la hipótesis nula y concluir que \textbf{el modelo presenta
heterocedasticidad}, es decir, la varianza de los errores no es
constante.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Ahora, procedemos a probar si los datos se distribuyen o no de manera
  normal con las dos pruebas de bondad de ajuste de D'Angostino y Jarque
  Bera.
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{336}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}

\PY{c+c1}{\PYZsh{}Test gráfico de normalidad}
\PY{n}{figure} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{residuos}\PY{p}{,} \PY{n}{line} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{gcf}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_196_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Al graficar los residuos, no nos da información aparente para descartar
la normalidad de los residuos. Además, el p-valor de la prueba Jarque
Bera es mayor a 0.05, lo que no nos da información suficiente para
rechazar la hipótesis nula ni descartar la distribución normal de los
residuos. Sin embargo, procedemos con el test D'Angostino

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{337}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{normaltest}

\PY{c+c1}{\PYZsh{}Test 2 de bondad de ajuste}
\PY{n}{k2}\PY{p}{,} \PY{n}{p\PYZus{}value} \PY{o}{=} \PY{n}{normaltest}\PY{p}{(}\PY{n}{residuos}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Estadístico = }\PY{l+s+si}{\PYZob{}}\PY{n}{k2}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{, p\PYZhy{}value = }\PY{l+s+si}{\PYZob{}}\PY{n}{p\PYZus{}value}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Estadístico = 5.233211197298024, p-value = 0.0730504048541871
    \end{Verbatim}

    Como vemos con el test de Jarque - Bera, el p - valor de la prueba de
D'Angostino está por encima del 0.05, lo que conlleva a aceptar la
hipótesis nula de normalidad de los residuos.

Ahora, hecha la validad de los supuestos, podemos plantearnos correr un
modelo robusto como Mínimos Cuadrados Generalizados y revisar si esto
nos corrige los errores aquí presentados, principalmente la
heterocedasticidad.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{338}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}r} \PY{o}{=} \PY{n}{concrete}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coarse Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fine Aggregate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{const}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{y\PYZus{}r} \PY{o}{=} \PY{n}{concrete}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Concrete Compressive Strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{model\PYZus{}r} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{GLS}\PY{p}{(}\PY{n}{y\PYZus{}r}\PY{p}{,} \PY{n}{X\PYZus{}r}\PY{p}{)}
\PY{n}{res} \PY{o}{=} \PY{n}{model\PYZus{}r}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{res}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
                                  GLS Regression Results
================================================================================
=========
Dep. Variable:     Concrete Compressive Strength   R-squared:
0.614
Model:                                       GLS   Adj. R-squared:
0.612
Method:                            Least Squares   F-statistic:
271.2
Date:                           Sun, 28 Apr 2024   Prob (F-statistic):
1.78e-207
Time:                                   19:33:47   Log-Likelihood:
-3871.0
No. Observations:                           1030   AIC:
7756.
Df Residuals:                               1023   BIC:
7791.
Df Model:                                      6
Covariance Type:                       nonrobust
================================================================================
======
                         coef    std err          t      P>|t|      [0.025
0.975]
--------------------------------------------------------------------------------
------
Cement                 0.1054      0.004     24.821      0.000       0.097
0.114
Blast Furnace Slag     0.0865      0.005     17.386      0.000       0.077
0.096
Fly Ash                0.0687      0.008      8.881      0.000       0.054
0.084
Water                 -0.2183      0.021    -10.332      0.000      -0.260
-0.177
Superplasticizer       0.2390      0.085      2.826      0.005       0.073
0.405
Age                    0.1135      0.005     20.987      0.000       0.103
0.124
const                 29.0302      4.212      6.891      0.000      20.764
37.296
==============================================================================
Omnibus:                        5.233   Durbin-Watson:                   1.286
Prob(Omnibus):                  0.073   Jarque-Bera (JB):                5.193
Skew:                          -0.174   Prob(JB):                       0.0745
Kurtosis:                       3.019   Cond. No.                     4.66e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly
specified.
[2] The condition number is large, 4.66e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
    \end{Verbatim}

    Al correr los mínimos cuadrados generalizados vemos que los estadísticos
del modelo no cambian:

\$ Adj - R\^{}2 = 0.612 \$ Lo que implica que el modelo no ajusta mejor
la variación de los datos. El modelo sigue siendo significativo para
explicar el comportamiento de la variable dependiente, pero los
criterios de infromación de Akaike no mejoran, lo que significa que este
modelo no es mejor al planteado en el apartado anterior.

En cuanto a los supuestos sobre el modelo, vemos que la métrica de
Durbin - Watson no se modifica, lo que implica que GLS no logra corregir
el problema de acutocorrelación. Se sigue cumpliendo Jarque - Bera.
Corremos las pruebas para Breusch - Pagan y White para validar
homocedasticidad.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{339}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{resid\PYZus{}r} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{resid}
\PY{c+c1}{\PYZsh{} Test 2}
\PY{n}{BP\PYZus{}r} \PY{o}{=} \PY{n}{het\PYZus{}breuschpagan}\PY{p}{(}\PY{n}{resid\PYZus{}r}\PY{p}{,} \PY{n}{res}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{BP\PYZus{}r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{BP\PYZus{}r}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El estadistico Breusch \PYZhy{} Pagan es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}r}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ y el p\PYZhy{}value es }\PY{l+s+si}{\PYZob{}}\PY{n}{BP\PYZus{}r}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
El estadistico Breusch - Pagan es 139.182 y el p-value es 0.0
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{340}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Test 3}
\PY{n}{white\PYZus{}test\PYZus{}r} \PY{o}{=} \PY{n}{het\PYZus{}white}\PY{p}{(}\PY{n}{resid\PYZus{}r}\PY{p}{,} \PY{n}{res}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{exog}\PY{p}{)}
\PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Estadístico White}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Statistic}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F\PYZhy{}Test p\PYZhy{}value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{white\PYZus{}test\PYZus{}r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{white\PYZus{}test\PYZus{}r}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} H0: homocedasticidad en los errores}
\PY{c+c1}{\PYZsh{} H1: heterocedasticiadad en los erroes}

\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{labels}\PY{p}{,} \PY{n}{white\PYZus{}test\PYZus{}r}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'Estadístico White': 258.69, 'p-value': 0.0, 'F-Statistic': 12.447, 'F-Test
p-value': 0.0\}
    \end{Verbatim}

    A pesar de correr el modelo generalizado, evidenciamos que ambas pruebas
nos dan un p-value menor a 0.05, lo que nos lleva nuevamente a rechazar
la hipótesis nula y concluir que \textbf{el modelo presenta
heterocedasticidad}, es decir, la varianza de los errores no es
constante.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
