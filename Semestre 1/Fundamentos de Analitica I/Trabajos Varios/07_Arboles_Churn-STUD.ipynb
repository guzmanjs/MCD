{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión: Churn compañía de servicios de telefonía móvil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar un modelo de clasificación de árboles de decisión a un dataset que describe los clientes de una compañía que presta servicios de telefonía móvil que se han abandonado o no la compañía para irse a la competencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #operaciones matriciales y con vectores\n",
    "import pandas as pd #tratamiento de datos\n",
    "import matplotlib.pyplot as plt #gráficos\n",
    "from sklearn import tree, datasets, metrics\n",
    "#from sklearn import neighbors, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split #metodo de particionamiento de datasets para evaluación\n",
    "from sklearn.model_selection import cross_val_score, cross_validate #método para evaluar varios particionamientos de C-V\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, LeaveOneOut #Iteradores de C-V\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendimiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos para entenderlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "embedded null character",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14012\\1615081408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\OneDrive - Tecnoquimicas\\99. PERSONAL\\Formación\\Maestria\\Semestre 1\\Fundamentos de Analitica I\\08-06-churn.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: embedded null character"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"D:\\OneDrive - Tecnoquimicas\\99. PERSONAL\\Formación\\Maestria\\Semestre 1\\Fundamentos de Analitica I\\08-06-churn.csv\", sep=';', na_values=\".\")\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 1**:\n",
    "* Determine el número de registros, de variables, sus tipos ideales/reales, y sus rangos\n",
    "* Determine el baseline y su accuracy. ¿Están balanceados los datos?\n",
    "* ¿Encuentran algún problema con los datos (missing values, datos inválidos, etc.)?\n",
    "* Si fuésemos a utilizar K-NN, ¿debería hacerse algún pretratamiento de los datos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método **describe** permite obtener un dataframe con una descripción de las variables de un dataframe analizado. Para cada variable encontramos el número de registros validos (*count*).\n",
    "\n",
    "Además, si se trata de una variable categórica, se puede obtener el número de clases posibles (*unique*), la clase mayoritaria (*top*) y la frecuencia de la clase mayoritaria (*freq*).\n",
    "\n",
    "Y, si se trata de una variable numérica, se puede obtener el promedio (*mean*), desviación estándar (*std*), los valores mínimos (*min*) y máximos (*max*) y los cuartiles (*25%*, *50%* y *75%*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LEAVE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LEAVE.describe()['freq'] / data.LEAVE.describe()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LEAVE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.LEAVE.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "RESPONDER AQUI A LAS PREGUNTAS DE LA PARTE 1.\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 2**:\n",
    "\n",
    "Utilizando pandas y matplotlib, analice la distribución de las variables independientes con respecto a los valores de la variable objetivo LEAVE y STAY. Trate de encontrar patrones en plots univariados (densidad) y Bivariados (scatterplots) para las variables numéricas, y gráficos de barras de conteo para las categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_indep_cat = ['COLLEGE', 'REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL',\n",
    "       'CONSIDERING_CHANGE_OF_PLAN', 'LEAVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var_cat in var_indep_cat:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_indep_cat)/2), 2, i)\n",
    "    sns.countplot(x=var_cat, hue=\"LEAVE\", data=data)\n",
    "    plt.title(var_cat)\n",
    "    plt.legend(['LEAVE', 'STAY'])\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_indep_cat.remove('LEAVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_indep_num = ['INCOME', 'OVERAGE', 'LEFTOVER', 'HOUSE', 'HANDSET_PRICE',\n",
    "                 'OVER_15MINS_CALLS_PER_MONTH', 'AVERAGE_CALL_DURATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var_num in var_indep_num:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_indep_num)/2), 2, i)\n",
    "    sns.kdeplot(data[data['LEAVE']=='LEAVE'][var_num], shade=True, color='r', ax=ax);\n",
    "    sns.kdeplot(data[data['LEAVE']=='STAY'][var_num], shade=True, color='g', ax=ax);\n",
    "    plt.title(var_num)\n",
    "    plt.legend(['LEAVE', 'STAY'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,20))\n",
    "ax = fig.add_subplot(3, 2, 1)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"INCOME\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. INCOME\")\n",
    "ax = fig.add_subplot(3, 2, 2)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"OVERAGE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. OVERAGE\")\n",
    "ax = fig.add_subplot(3, 2, 3)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"LEFTOVER\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. LEFTOVER\")\n",
    "ax = fig.add_subplot(3, 2, 4)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"HANDSET_PRICE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. HANDSET_PRICE\")\n",
    "ax = fig.add_subplot(3, 2, 5)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"OVER_15MINS_CALLS_PER_MONTH\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. OVER_15MINS_CALLS_PER_MONTH\")\n",
    "ax = fig.add_subplot(3, 2, 6)\n",
    "sns.scatterplot(x=\"HOUSE\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HOUSE vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,20))\n",
    "ax = fig.add_subplot(3, 2, 1)\n",
    "sns.scatterplot(x=\"INCOME\", y=\"OVERAGE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"INCOME vs. OVERAGE\")\n",
    "ax = fig.add_subplot(3, 2, 2)\n",
    "sns.scatterplot(x=\"INCOME\", y=\"LEFTOVER\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"INCOME vs. LEFTOVER\")\n",
    "ax = fig.add_subplot(3, 2, 3)\n",
    "sns.scatterplot(x=\"INCOME\", y=\"HANDSET_PRICE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"INCOME vs. HANDSET_PRICE\")\n",
    "ax = fig.add_subplot(3, 2, 4)\n",
    "sns.scatterplot(x=\"INCOME\", y=\"OVER_15MINS_CALLS_PER_MONTH\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"INCOME vs. OVER_15MINS_CALLS_PER_MONTH\")\n",
    "ax = fig.add_subplot(3, 2, 5)\n",
    "sns.scatterplot(x=\"INCOME\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"INCOME vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.scatterplot(x=\"OVERAGE\", y=\"LEFTOVER\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"OVERAGE vs. LEFTOVER\")\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.scatterplot(x=\"OVERAGE\", y=\"HANDSET_PRICE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"OVERAGE vs. HANDSET_PRICE\")\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"OVERAGE\", y=\"OVER_15MINS_CALLS_PER_MONTH\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"OVERAGE vs. OVER_15MINS_CALLS_PER_MONTH\")\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "sns.scatterplot(x=\"OVERAGE\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"OVERAGE vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.scatterplot(x=\"LEFTOVER\", y=\"HANDSET_PRICE\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"LEFTOVER vs. HANDSET_PRICE\")\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.scatterplot(x=\"LEFTOVER\", y=\"OVER_15MINS_CALLS_PER_MONTH\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"LEFTOVER vs. OVER_15MINS_CALLS_PER_MONTH\")\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"LEFTOVER\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"LEFTOVER vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"HANDSET_PRICE\", y=\"OVER_15MINS_CALLS_PER_MONTH\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HANDSET_PRICE vs. OVER_15MINS_CALLS_PER_MONTH\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "sns.scatterplot(x=\"HANDSET_PRICE\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"HANDSET_PRICE vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los plots no son fáciles de interpretar porque los puntos están uno encima de otro.\n",
    "Para estos casos se puede agregar un poco de ruido (**jitter**) a los puntos. \n",
    "Vamos a agregarlo a mano para el siguiente plot (increiblemente Python no tiene esta funcionalidad!!!).\n",
    "Veamos como sería sin el jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"OVER_15MINS_CALLS_PER_MONTH\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=data, ax=ax)\n",
    "plt.title(\"OVER_15MINS_CALLS_PER_MONTH vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos con el jitter (creado a mano), combinado con una reducción en el tamaño de los puntos para que se vea una mayor cantidad de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = data[[\"OVER_15MINS_CALLS_PER_MONTH\", \"AVERAGE_CALL_DURATION\", \"LEAVE\"]]\n",
    "jitter = 0.3\n",
    "d2.OVER_15MINS_CALLS_PER_MONTH = data.OVER_15MINS_CALLS_PER_MONTH + np.random.normal(scale=jitter, size=20000)\n",
    "d2.AVERAGE_CALL_DURATION = data.AVERAGE_CALL_DURATION + np.random.normal(scale=jitter, size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"OVER_15MINS_CALLS_PER_MONTH\", y=\"AVERAGE_CALL_DURATION\", hue=\"LEAVE\", data=d2, ax=ax, size=1)\n",
    "plt.title(\"OVER_15MINS_CALLS_PER_MONTH vs. AVERAGE_CALL_DURATION\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación a partir un árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de decisión permiten utilizar tanto las variables predictivas categóricas como las numéricas.\n",
    "No todas las variables van a ser útiles. El árbol se va a encargar de encontrar la mejor variable a utilizar en el contexto del subconjunto de datos de cada rama.\n",
    "\n",
    "Veamos cómo se crea un árbol de decisión en scikit-learn, y algunos de los parámetros más importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos (gini, entropy)\n",
    "    max_depth=None,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "    min_samples_split=2, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "    min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "    max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight=None    # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota editorial**: scikit-learn soporta en sus árboles de decisión solamente variables independientes numéricas!!!!! Toca entonces utilizar un encoding (one hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_one_hot = pd.get_dummies(data[var_indep_cat], prefix=var_indep_cat)\n",
    "X = data[var_indep_num].join(data_cat_one_hot)\n",
    "y = data['LEAVE']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quedaron 24 variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez el árbol es aprendido, se puede consultar diferentes atributos.\n",
    "El mas interesante, aparte del árbol en sí, es el que asocia un índice de importancia a los atributos independientes en la clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos entonces que las variables en orden de importancia son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[np.argsort(-ctree.feature_importances_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a visualizar el árbol aprendido (para hacerlo, se necesita preinstalar la aplicación graphviz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(ctree, \n",
    "                     filled=True, rounded=True,  #nodos redondeados y coloreados\n",
    "                     class_names=ctree.classes_,\n",
    "                     feature_names=X_train.columns,  \n",
    "                     out_file=dot_data,\n",
    "                     special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imágen es demasiado grande para poder visualizarla aquí, vamos a guardarla un archivo y abrirla en un visor externo que permita hacer zoom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PNG\n",
    "graph.write_png(\"arbol.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver el árbol es increiblemente grande y complejo, pues no se especificó ninguna manera de limitar su crecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos el modelo \"aprendido\" con el dataset de 20000 instancias.\n",
    "Vamos ahora a evaluarlo sobre ese mismo dataset para poder ver los éxitos y errores de la predicción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ctree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title(\"Matriz de confusión para K=5\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, y_test.unique())\n",
    "plt.yticks(tick_marks, y_test.unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Precisión     : \", metrics.precision_score(y_test, y_pred, \"LEAVE\", average='macro'))\n",
    "print(\"Recall        : \", metrics.recall_score(y_test, y_pred, \"LEAVE\", average='macro'))\n",
    "VN = np.sum(cm[1:3,1:3])\n",
    "FP = np.sum(cm[0,1:3])\n",
    "specificity = VN/(VN+FP)\n",
    "print(\"Especificidad : \", specificity)\n",
    "print(\"F1-score      : \", metrics.f1_score(y_test, y_pred, \"LEAVE\", average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el árbol completo, obtuvimos un nivel de accuracy del 61.68%, cuando el baseline era de 50.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting: poda del árbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver que hubiera pasado si limitamos el crecimiento del árbol (prepoda), controlando la profundidad del árbol y el mínimo número de registros de un nodo para permitir el particionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parte 3**:\n",
    "\n",
    "Modifiquen el parámetro **criterion** y los parámetros de pre-poda buscando una mejor exactitud del modelo:\n",
    "- **max_depth**: entre mas grande el valor, el árbol será más complejo (más número de niveles de profundidad)\n",
    "- **min_samples_split**: entre mas grande el valor, el árbol será más sencillo (se necesita tener más registros en un nodo para poder particionarlo)\n",
    "- **min_samples_leaf**: entre mas grande el valor, el árbol será más sencillo (se necesita tener más registros en una hoja para poder aceptarla, si no se llega a esa cardinalidad, no se permite el partionamiento de su nodo padre) \n",
    "- **min_impurity_decrease**: entre mas grande el valor, el árbol será más sencillo (un nivel de impureza bajo inferior a este umbral no desatará un particionamiento. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "#    max_depth=,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "#    min_samples_split=, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "#    min_samples_leaf=,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "#    max_leaf_nodes=, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight=None    # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")\n",
    "ctree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ctree.predict(X_test)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(ctree, \n",
    "                     filled=True, rounded=True,  #nodos redondeados y coloreados\n",
    "                     class_names=ctree.classes_,\n",
    "                     feature_names=X_train.columns,  \n",
    "                     out_file=dot_data,\n",
    "                     special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos seguir mejorando el árbol buscando un tuning de la prepoda mas complejo con los demás parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos intentar con otro tipo de modelos, e.g KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(1234)\n",
    "knn = KNeighborsClassifier(n_neighbors=300)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También con un modelo Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "np.random.seed(1234)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de ensamble: bagging, random forest, boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a crear un modelo de ensamble que utiliza muchos modelos de árboles sencillos que pone a votar para encontrar una decisión consensuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "np.random.seed(1234)\n",
    "ctree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "    max_depth=7,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "    min_samples_split=1000, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "    min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "    random_state=None,   # semilla del generador aleatorio utilizado para \n",
    "    max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "    min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "    class_weight=None    # permite asociar pesos a las clases, en el caso de diferencias de importancia entre ellas\n",
    ")\n",
    "bagging = BaggingClassifier(base_estimator = ctree,   # Por defecto un decision tree \n",
    "                            n_estimators=200,          # Número de modelos a crear\n",
    "                            max_samples=0.7,          # Número o % de registros de la muestra de aprendizaje\n",
    "                            max_features=0.7,         # Número o % de atributos de la muestra de aprendizaje\n",
    "                            bootstrap=True,           # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                            bootstrap_features=False, # Utilizar reemplazo en el muestreo de los atributos de aprendizaje\n",
    "                            oob_score=False,          # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                            n_jobs=2,                 # Número de cores a utilizar\n",
    "                            random_state=None,        # random seed para el generador aleatorio\n",
    "                            verbose=0)                # controla la cantidad de información a reportar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer bagging de otro tipo de estimadores de base, por ejemplo KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(1234)\n",
    "bagging = BaggingClassifier(base_estimator = KNeighborsClassifier(),   # Por defecto un decision tree \n",
    "                            n_estimators=50,          # Número de modelos a crear\n",
    "                            max_samples=0.5,          # Número o % de registros de la muestra de aprendizaje\n",
    "                            max_features=0.5,         # Número o % de atributos de la muestra de aprendizaje\n",
    "                            bootstrap=True,           # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                            bootstrap_features=False, # Utilizar reemplazo en el muestreo de los atributos de aprendizaje\n",
    "                            oob_score=False,          # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                            n_jobs=2,                 # Número de cores a utilizar\n",
    "                            random_state=None,        # random seed para el generador aleatorio\n",
    "                            verbose=0)                # controla la cantidad de información a reportar\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred = bagging.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(1234)\n",
    "rforest = RandomForestClassifier(n_estimators=100,     #Número de modelos a crear \n",
    "                                criterion='entropy', # el criterio de particionamiento de un conjunto de datos\n",
    "                                max_depth=6,      # prepoda: controla la profundidad del árbol (largo máximo de las ramas)\n",
    "                                min_samples_split=1000, # prepoda: el mínimo número de registros necesarios para crear una nueva rama\n",
    "                                min_samples_leaf=1,  # prepoda: el mínimo número de registros en una hoja\n",
    "                                max_features=('auto'), # Número o % de atributos de la muestra de aprendizaje\n",
    "                                max_leaf_nodes=None, # prepoda: máximo número de nodos hojas\n",
    "                                min_impurity_decrease=0.0, # prepoda: umbral mínimo de reducción de la impureza para aceptar la creación de una rama\n",
    "                                bootstrap=True,      # Utilizar reemplazo en el muestreo de los registros de aprendizaje\n",
    "                                oob_score=True,     # Evaluar cada modelo con los registros no utilizados en su aprendizaje \n",
    "                                n_jobs=2,            # Número de cores a utilizar\n",
    "                                random_state=None,   # random seed para el generador aleatorio\n",
    "                                verbose=0)           # controla la cantidad de información a reportar\n",
    "rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "cm= metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Exactitud: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Kappa    : \", metrics.cohen_kappa_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que las variables en orden de importancia son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[np.argsort(-rforest.feature_importances_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "np.random.seed(1234)\n",
    "\n",
    "numModelos = 400\n",
    "ada_10 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                         algorithm='SAMME',\n",
    "                         learning_rate=1.0)    #Reduce la importancia de los modelos mas recientes\n",
    "ada_10.fit(X_train, y_train)\n",
    "ada_02 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                        algorithm='SAMME',\n",
    "                         learning_rate=0.2)    #Reduce la importancia de los modelos mas recientes\n",
    "\n",
    "ada_02.fit(X_train, y_train)\n",
    "ada_005 = AdaBoostClassifier(base_estimator=None,  #Por defecto se trata de decision stumps\n",
    "                         n_estimators=numModelos,     #Número de modelos a crear\n",
    "                         algorithm='SAMME',\n",
    "                         learning_rate=0.05)    #Reduce la importancia de los modelos mas recientes\n",
    "ada_005.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_errores_test_10 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_10.staged_predict(X_test)):\n",
    "    ada_errores_test_10[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "ada_errores_train_10 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_10.staged_predict(X_train)):\n",
    "    ada_errores_train_10[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_errores_test_02 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_02.staged_predict(X_test)):\n",
    "    ada_errores_test_02[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "ada_errores_train_02 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_02.staged_predict(X_train)):\n",
    "    ada_errores_train_02[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_errores_test_005 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_005.staged_predict(X_test)):\n",
    "    ada_errores_test_005[i] = 1-zero_one_loss(y_pred, y_test)\n",
    "\n",
    "ada_errores_train_005 = np.zeros((numModelos,))\n",
    "for i, y_pred in enumerate(ada_005.staged_predict(X_train)):\n",
    "    ada_errores_train_005[i] = 1-zero_one_loss(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_10,\n",
    "        label='AdaBoost Test Acc lr 1.0',\n",
    "        color='red')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_10,\n",
    "        label=' AdaBoost Train Acc lr 1.0',\n",
    "        color='blue')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_02,\n",
    "        label='AdaBoost Test Acc lr 0.2',\n",
    "        color='gray')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_02,\n",
    "        label=' AdaBoost Train Acc lr 0.2',\n",
    "        color='cyan')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_test_005,\n",
    "        label='AdaBoost Test Acc lr 0.05',\n",
    "        color='green')\n",
    "ax.plot(np.arange(numModelos) + 1, ada_errores_train_005,\n",
    "        label=' AdaBoost Train Acc lr 0.05',\n",
    "        color='yellow')\n",
    "\n",
    "ax.set_ylim((0.6, 0.7))\n",
    "ax.set_xlabel('# de modelos')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "leg = ax.legend(loc='lower right', fancybox=True)\n",
    "leg.get_frame().set_alpha(0.7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
